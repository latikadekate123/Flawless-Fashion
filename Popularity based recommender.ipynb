{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a737d3d-e7fd-48a2-bc24-ec95aea85c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('alo_yoga_products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8b84ad-d3b8-46e2-a641-5929322f4b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>current_color</th>\n",
       "      <th>price</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>badge</th>\n",
       "      <th>image_url</th>\n",
       "      <th>product_url</th>\n",
       "      <th>colors_available</th>\n",
       "      <th>sizes_available</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_count</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Airlift Intrigue Bra - Espresso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w9679r-airlif...</td>\n",
       "      <td>Espresso, 12, 12, Espresso, Black, Navy, Anthr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:02:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/8 High-Waist Airlift Legging - Espresso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w51314r-7-8-h...</td>\n",
       "      <td>Espresso, 12, 12, Espresso, Black, Navy, Anthr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Work It Mini Skirt - Green Olive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w6503r-work-i...</td>\n",
       "      <td>Green Olive, Green Olive, Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Work It Bomber - Green Olive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w4689r-work-i...</td>\n",
       "      <td>Green Olive, Green Olive, Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Airlift Intrigue Bra - Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w9679r-airlif...</td>\n",
       "      <td>Black, 12, 12, Black, Espresso, Navy, Anthraci...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>Airlift Strength Bra - Navy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w9724r-airlif...</td>\n",
       "      <td>Navy, Navy, Black, Espresso, Bold Red</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:09:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>Airlift Advantage Racerback Bra - Spearmint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w9555r-airlif...</td>\n",
       "      <td>Spearmint, Spearmint, Black, White, Macadamia,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:09:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>Airlift Headband - Lunar Grey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/a0054u-airlif...</td>\n",
       "      <td>Lunar Grey, Lunar Grey, Green Olive, Neon Bubb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:09:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>Polar Fleece Wintry Mix Skirt - Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w6476r-polar-...</td>\n",
       "      <td>Black, Black, Ivory, Charcoal Green</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:09:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>Stretch Woven Street Puffer - Bone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/pr...</td>\n",
       "      <td>https://www.aloyoga.com/products/m4134r-stretc...</td>\n",
       "      <td>Bone, Bone, Eclipse Blue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-06 14:09:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name  current_color price  \\\n",
       "0                Airlift Intrigue Bra - Espresso            NaN   $68   \n",
       "1      7/8 High-Waist Airlift Legging - Espresso            NaN  $128   \n",
       "2               Work It Mini Skirt - Green Olive            NaN   $98   \n",
       "3                   Work It Bomber - Green Olive            NaN  $228   \n",
       "4                   Airlift Intrigue Bra - Black            NaN   $68   \n",
       "..                                           ...            ...   ...   \n",
       "715                  Airlift Strength Bra - Navy            NaN   $78   \n",
       "716  Airlift Advantage Racerback Bra - Spearmint            NaN   $68   \n",
       "717                Airlift Headband - Lunar Grey            NaN   $34   \n",
       "718        Polar Fleece Wintry Mix Skirt - Black            NaN   $98   \n",
       "719           Stretch Woven Street Puffer - Bone            NaN  $228   \n",
       "\n",
       "     sale_price  badge                                          image_url  \\\n",
       "0           NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "1           NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "2           NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "3           NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "4           NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "..          ...    ...                                                ...   \n",
       "715         NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "716         NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "717         NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "718         NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "719         NaN    NaN  https://cdn.shopify.com/s/files/1/2185/2813/pr...   \n",
       "\n",
       "                                           product_url  \\\n",
       "0    https://www.aloyoga.com/products/w9679r-airlif...   \n",
       "1    https://www.aloyoga.com/products/w51314r-7-8-h...   \n",
       "2    https://www.aloyoga.com/products/w6503r-work-i...   \n",
       "3    https://www.aloyoga.com/products/w4689r-work-i...   \n",
       "4    https://www.aloyoga.com/products/w9679r-airlif...   \n",
       "..                                                 ...   \n",
       "715  https://www.aloyoga.com/products/w9724r-airlif...   \n",
       "716  https://www.aloyoga.com/products/w9555r-airlif...   \n",
       "717  https://www.aloyoga.com/products/a0054u-airlif...   \n",
       "718  https://www.aloyoga.com/products/w6476r-polar-...   \n",
       "719  https://www.aloyoga.com/products/m4134r-stretc...   \n",
       "\n",
       "                                      colors_available  sizes_available  \\\n",
       "0    Espresso, 12, 12, Espresso, Black, Navy, Anthr...              NaN   \n",
       "1    Espresso, 12, 12, Espresso, Black, Navy, Anthr...              NaN   \n",
       "2                      Green Olive, Green Olive, Black              NaN   \n",
       "3                      Green Olive, Green Olive, Black              NaN   \n",
       "4    Black, 12, 12, Black, Espresso, Navy, Anthraci...              NaN   \n",
       "..                                                 ...              ...   \n",
       "715              Navy, Navy, Black, Espresso, Bold Red              NaN   \n",
       "716  Spearmint, Spearmint, Black, White, Macadamia,...              NaN   \n",
       "717  Lunar Grey, Lunar Grey, Green Olive, Neon Bubb...              NaN   \n",
       "718                Black, Black, Ivory, Charcoal Green              NaN   \n",
       "719                           Bone, Bone, Eclipse Blue              NaN   \n",
       "\n",
       "     rating  review_count           scraped_at  \n",
       "0       NaN           NaN  2025-09-06 14:02:15  \n",
       "1       NaN           NaN  2025-09-06 14:02:16  \n",
       "2       NaN           NaN  2025-09-06 14:02:17  \n",
       "3       NaN           NaN  2025-09-06 14:02:17  \n",
       "4       NaN           NaN  2025-09-06 14:02:18  \n",
       "..      ...           ...                  ...  \n",
       "715     NaN           NaN  2025-09-06 14:09:40  \n",
       "716     NaN           NaN  2025-09-06 14:09:40  \n",
       "717     NaN           NaN  2025-09-06 14:09:41  \n",
       "718     NaN           NaN  2025-09-06 14:09:42  \n",
       "719     NaN           NaN  2025-09-06 14:09:42  \n",
       "\n",
       "[720 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ce97b2-43f3-4206-9104-c482b2cacb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_color</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>badge</th>\n",
       "      <th>sizes_available</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       current_color  sale_price  badge  sizes_available  rating  review_count\n",
       "count            0.0         0.0    0.0              0.0     0.0           0.0\n",
       "mean             NaN         NaN    NaN              NaN     NaN           NaN\n",
       "std              NaN         NaN    NaN              NaN     NaN           NaN\n",
       "min              NaN         NaN    NaN              NaN     NaN           NaN\n",
       "25%              NaN         NaN    NaN              NaN     NaN           NaN\n",
       "50%              NaN         NaN    NaN              NaN     NaN           NaN\n",
       "75%              NaN         NaN    NaN              NaN     NaN           NaN\n",
       "max              NaN         NaN    NaN              NaN     NaN           NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d7c362-09de-4d35-b0b6-e0c45691dd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Product Categories</th>\n",
       "      <th>Avg Price</th>\n",
       "      <th>Brands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alo_yoga_products.csv</td>\n",
       "      <td>—</td>\n",
       "      <td>$125.45</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>altardstate_products.csv</td>\n",
       "      <td>Clothing, Clothing/Dresses</td>\n",
       "      <td>$68.57</td>\n",
       "      <td>ALTAR'D STATE, AS REVIVAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cupshe_products.csv</td>\n",
       "      <td>—</td>\n",
       "      <td>$36.64</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>edikted_products.csv</td>\n",
       "      <td>—</td>\n",
       "      <td>$25.32</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gymshark_products.csv</td>\n",
       "      <td>—</td>\n",
       "      <td>$42.48</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nakd_products.csv</td>\n",
       "      <td></td>\n",
       "      <td>$69.89</td>\n",
       "      <td>Camelia Farhoodi x NA-KD, Cecilie Haugaard x N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>princess_polly.csv</td>\n",
       "      <td>—</td>\n",
       "      <td>$56.94</td>\n",
       "      <td>Lioness, Motel, Princess Polly, Princess Polly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vuori_products.csv</td>\n",
       "      <td>—</td>\n",
       "      <td>$92.70</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       File          Product Categories Avg Price  \\\n",
       "0     alo_yoga_products.csv                           —   $125.45   \n",
       "1  altardstate_products.csv  Clothing, Clothing/Dresses    $68.57   \n",
       "2       cupshe_products.csv                           —    $36.64   \n",
       "3      edikted_products.csv                           —    $25.32   \n",
       "4     gymshark_products.csv                           —    $42.48   \n",
       "5         nakd_products.csv                                $69.89   \n",
       "6        princess_polly.csv                           —    $56.94   \n",
       "7        vuori_products.csv                           —    $92.70   \n",
       "\n",
       "                                              Brands  \n",
       "0                                                  —  \n",
       "1                          ALTAR'D STATE, AS REVIVAL  \n",
       "2                                                  —  \n",
       "3                                                  —  \n",
       "4                                                  —  \n",
       "5  Camelia Farhoodi x NA-KD, Cecilie Haugaard x N...  \n",
       "6  Lioness, Motel, Princess Polly, Princess Polly...  \n",
       "7                                                  —  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------ config ------------------\n",
    "FILES = [\n",
    "    \"alo_yoga_products.csv\",\n",
    "    \"altardstate_products.csv\",\n",
    "    \"cupshe_products.csv\",\n",
    "    \"edikted_products.csv\",\n",
    "    \"gymshark_products.csv\",\n",
    "    \"nakd_products.csv\",\n",
    "    \"princess_polly.csv\",\n",
    "    \"vuori_products.csv\",\n",
    "]\n",
    "DATA_DIR = Path(\"./\")  # folder with your CSVs\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out.columns = [re.sub(r\"\\s+\", \"_\", c.strip().lower()) for c in out.columns]\n",
    "    return out\n",
    "\n",
    "RANGE_SEPS = r\"[-–—to]+\"  # -, en dash, em dash, or the word 'to'\n",
    "\n",
    "def parse_price_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return a single numeric 'price' series.\n",
    "    Handles $, commas, and ranges like '40-60' or '$40–$60' (uses midpoint).\n",
    "    \"\"\"\n",
    "    txt = s.astype(str).str.strip()\n",
    "    txt = txt.str.replace(r\"[^\\d\\.,\\-–—to]\", \"\", regex=True).str.replace(\",\", \"\", regex=False)\n",
    "    parts = txt.str.split(rf\"\\s*{RANGE_SEPS}\\s*\", n=1, regex=True)\n",
    "\n",
    "    pmin = parts.apply(lambda x: x[0].strip() if isinstance(x, list) and len(x) >= 1 else np.nan)\n",
    "    pmax = parts.apply(lambda x: x[1].strip() if isinstance(x, list) and len(x) >= 2 else np.nan)\n",
    "\n",
    "    pmin = pd.to_numeric(pmin, errors=\"coerce\")\n",
    "    pmax = pd.to_numeric(pmax, errors=\"coerce\")\n",
    "\n",
    "    price = pmin.copy()\n",
    "    both = pmin.notna() & pmax.notna()\n",
    "    price[both] = (pmin[both] + pmax[both]) / 2.0\n",
    "    return price\n",
    "\n",
    "def infer_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ------------------ extraction ------------------\n",
    "rows = []\n",
    "for fname in FILES:\n",
    "    df = pd.read_csv(DATA_DIR / fname)\n",
    "    df = normalize_cols(df)\n",
    "\n",
    "    # infer likely columns\n",
    "    cat_col   = infer_col(df, [\"category\",\"product_category\",\"product_type\",\"type\",\"categories\",\"taxonomy\"])\n",
    "    price_col = infer_col(df, [\"price\",\"sale_price\",\"current_price\",\"list_price\",\"final_price\",\"price_usd\"])\n",
    "    brand_col = infer_col(df, [\"brand\",\"brand_name\",\"vendor\",\"label\"])\n",
    "\n",
    "    # categories\n",
    "    if cat_col:\n",
    "        cats = sorted(pd.Series(df[cat_col]).dropna().astype(str).unique())\n",
    "        # keep it compact for the table; tweak join as you like\n",
    "        categories_str = \", \".join(cats[:15]) + (\"…\" if len(cats) > 15 else \"\")\n",
    "    else:\n",
    "        categories_str = \"—\"  # missing in this file (you may replace with your curated list)\n",
    "\n",
    "    # price\n",
    "    if price_col:\n",
    "        price_num = parse_price_series(df[price_col])\n",
    "        avg_price = round(price_num.mean(skipna=True), 2) if price_num.notna().any() else None\n",
    "    else:\n",
    "        avg_price = None\n",
    "\n",
    "    # brands\n",
    "    if brand_col:\n",
    "        brands = sorted(pd.Series(df[brand_col]).dropna().astype(str).unique())\n",
    "        brands_str = \", \".join(brands[:15]) + (\"…\" if len(brands) > 15 else \"\")\n",
    "    else:\n",
    "        brands_str = \"—\"\n",
    "\n",
    "    rows.append({\n",
    "        \"File\": fname,\n",
    "        \"Product Categories\": categories_str,\n",
    "        \"Avg Price\": f\"${avg_price:.2f}\" if isinstance(avg_price, (int,float)) else \"—\",\n",
    "        \"Brands\": brands_str\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7720510c-bc48-4fe9-9d42-2f3141a72149",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cleaned/all_products_master.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# If you ran the cleaning step already, use the master it created:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m MASTER \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned/all_products_master.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(MASTER)\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# --- 1) Your curated Product Categories per file (from your table) ---\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned/all_products_master.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# If you ran the cleaning step already, use the master it created:\n",
    "MASTER = Path(\"cleaned/all_products_master.csv\")\n",
    "df = pd.read_csv(MASTER)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# --- 1) Your curated Product Categories per file (from your table) ---\n",
    "CURATED_CATS = {\n",
    "    \"alo_yoga_products.csv\": \"Bras, Leggings, Skirts, Bombers, Coverups, Shorts, Pullovers, Sweatpants, Tees\",\n",
    "    \"altardstate_products.csv\": \"Dresses (Maxi, Midi, Mini)\",\n",
    "    \"cupshe_products.csv\": \"Bikini Sets, One-Piece Swimsuits\",\n",
    "    \"edikted_products.csv\": \"Sweaters, Sweatpants, Jeans, Tops, Mini Skirts, Shorts, Skorts\",\n",
    "    \"gymshark_products.csv\": \"Sports Bras, Crop Tops, Leggings, Shorts, One-Pieces\",\n",
    "    \"nakd_products.csv\": \"Coats, Jackets, Blazers, Jeans, Tops, Sweater, Boots, Ballerinas, Trenchcoats\",\n",
    "    \"princess_polly.csv\": \"Mini Dresses\",\n",
    "    \"vuori_products.csv\": \"Pants, Leggings, Tees, Joggers, Tops, Jackets, Tank, Cardigan, Hoodie, Trouser, Bra, Short\",\n",
    "}\n",
    "\n",
    "# --- 2) Avg price per file (computed from numeric 'price' in the cleaned master) ---\n",
    "avg_price = (\n",
    "    df.groupby(\"source_file\", dropna=False)[\"price\"]\n",
    "      .mean()\n",
    "      .round(2)\n",
    "      .rename(\"Avg Price\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# --- 3) Brand list per file (if 'brand' exists) ---\n",
    "def uniq_join(series, limit=None):\n",
    "    vals = sorted(set(str(x) for x in series.dropna() if str(x).strip()))\n",
    "    if limit and len(vals) > limit:\n",
    "        return \", \".join(vals[:limit]) + \"...\"\n",
    "    return \", \".join(vals) if vals else \"—\"\n",
    "\n",
    "brands = (\n",
    "    df.groupby(\"source_file\", dropna=False)[\"brand\"]\n",
    "      .apply(lambda s: uniq_join(s, limit=None))  # set limit=15 if you want to truncate\n",
    "      .rename(\"Brands\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# --- 4) Build summary table in your desired order ---\n",
    "FILES = [\n",
    "    \"alo_yoga_products.csv\",\n",
    "    \"altardstate_products.csv\",\n",
    "    \"cupshe_products.csv\",\n",
    "    \"edikted_products.csv\",\n",
    "    \"gymshark_products.csv\",\n",
    "    \"nakd_products.csv\",\n",
    "    \"princess_polly.csv\",\n",
    "    \"vuori_products.csv\",\n",
    "]\n",
    "\n",
    "summary = pd.DataFrame({\"File\": FILES})\n",
    "summary[\"Product Categories\"] = summary[\"File\"].map(CURATED_CATS)\n",
    "\n",
    "summary = (summary\n",
    "           .merge(avg_price.rename(columns={\"source_file\": \"File\"}), on=\"File\", how=\"left\")\n",
    "           .merge(brands.rename(columns={\"source_file\": \"File\"}), on=\"File\", how=\"left\"))\n",
    "\n",
    "# Format Avg Price like $00.00 and fill missing with —\n",
    "summary[\"Avg Price\"] = summary[\"Avg Price\"].apply(lambda x: f\"${x:0.2f}\" if pd.notna(x) else \"—\")\n",
    "summary[\"Brands\"] = summary[\"Brands\"].fillna(\"—\")\n",
    "\n",
    "# Show full text (no truncation)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1fc4e7-9cc8-479b-8c0d-d86dda0472e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Product Categories</th>\n",
       "      <th>Avg Price</th>\n",
       "      <th>Brands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alo_yoga_products.csv</td>\n",
       "      <td>Bras, Leggings, Skirts, Bombers, Coverups, Sho...</td>\n",
       "      <td>$125.45</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>altardstate_products.csv</td>\n",
       "      <td>Dresses (Maxi, Midi, Mini)</td>\n",
       "      <td>$68.57</td>\n",
       "      <td>ALTAR'D STATE, AS REVIVAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cupshe_products.csv</td>\n",
       "      <td>Bikini Sets, One-Piece Swimsuits</td>\n",
       "      <td>$36.64</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>edikted_products.csv</td>\n",
       "      <td>Sweaters, Sweatpants, Jeans, Tops, Mini Skirts...</td>\n",
       "      <td>$25.32</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gymshark_products.csv</td>\n",
       "      <td>Sports Bras, Crop Tops, Leggings, Shorts, One-...</td>\n",
       "      <td>$42.48</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nakd_products.csv</td>\n",
       "      <td>Coats, Jackets, Blazers, Jeans, Tops, Sweater,...</td>\n",
       "      <td>$69.89</td>\n",
       "      <td>Camelia Farhoodi x NA-KD, Cecilie Haugaard x N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>princess_polly.csv</td>\n",
       "      <td>Mini Dresses</td>\n",
       "      <td>$56.94</td>\n",
       "      <td>Lioness, Motel, Princess Polly, Princess Polly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vuori_products.csv</td>\n",
       "      <td>Pants, Leggings, Tees, Joggers, Tops, Jackets,...</td>\n",
       "      <td>$92.70</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       File  \\\n",
       "0     alo_yoga_products.csv   \n",
       "1  altardstate_products.csv   \n",
       "2       cupshe_products.csv   \n",
       "3      edikted_products.csv   \n",
       "4     gymshark_products.csv   \n",
       "5         nakd_products.csv   \n",
       "6        princess_polly.csv   \n",
       "7        vuori_products.csv   \n",
       "\n",
       "                                  Product Categories Avg Price  \\\n",
       "0  Bras, Leggings, Skirts, Bombers, Coverups, Sho...   $125.45   \n",
       "1                         Dresses (Maxi, Midi, Mini)    $68.57   \n",
       "2                   Bikini Sets, One-Piece Swimsuits    $36.64   \n",
       "3  Sweaters, Sweatpants, Jeans, Tops, Mini Skirts...    $25.32   \n",
       "4  Sports Bras, Crop Tops, Leggings, Shorts, One-...    $42.48   \n",
       "5  Coats, Jackets, Blazers, Jeans, Tops, Sweater,...    $69.89   \n",
       "6                                       Mini Dresses    $56.94   \n",
       "7  Pants, Leggings, Tees, Joggers, Tops, Jackets,...    $92.70   \n",
       "\n",
       "                                              Brands  \n",
       "0                                                  —  \n",
       "1                          ALTAR'D STATE, AS REVIVAL  \n",
       "2                                                  —  \n",
       "3                                                  —  \n",
       "4                                                  —  \n",
       "5  Camelia Farhoodi x NA-KD, Cecilie Haugaard x N...  \n",
       "6  Lioness, Motel, Princess Polly, Princess Polly...  \n",
       "7                                                  —  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"./\")  # your notebook + csvs in same folder\n",
    "\n",
    "FILES = [\n",
    "    \"alo_yoga_products.csv\",\n",
    "    \"altardstate_products.csv\",\n",
    "    \"cupshe_products.csv\",\n",
    "    \"edikted_products.csv\",\n",
    "    \"gymshark_products.csv\",\n",
    "    \"nakd_products.csv\",\n",
    "    \"princess_polly.csv\",\n",
    "    \"vuori_products.csv\",\n",
    "]\n",
    "\n",
    "CURATED_CATS = {\n",
    "    \"alo_yoga_products.csv\": \"Bras, Leggings, Skirts, Bombers, Coverups, Shorts, Pullovers, Sweatpants, Tees\",\n",
    "    \"altardstate_products.csv\": \"Dresses (Maxi, Midi, Mini)\",\n",
    "    \"cupshe_products.csv\": \"Bikini Sets, One-Piece Swimsuits\",\n",
    "    \"edikted_products.csv\": \"Sweaters, Sweatpants, Jeans, Tops, Mini Skirts, Shorts, Skorts\",\n",
    "    \"gymshark_products.csv\": \"Sports Bras, Crop Tops, Leggings, Shorts, One-Pieces\",\n",
    "    \"nakd_products.csv\": \"Coats, Jackets, Blazers, Jeans, Tops, Sweater, Boots, Ballerinas, Trenchcoats\",\n",
    "    \"princess_polly.csv\": \"Mini Dresses\",\n",
    "    \"vuori_products.csv\": \"Pants, Leggings, Tees, Joggers, Tops, Jackets, Tank, Cardigan, Hoodie, Trouser, Bra, Short\",\n",
    "}\n",
    "\n",
    "RANGE_SEPS = r\"[-–—to]+\"\n",
    "\n",
    "def parse_price_series(s: pd.Series) -> pd.Series:\n",
    "    txt = s.astype(str).str.strip()\n",
    "    txt = txt.str.replace(r\"[^\\d\\.,\\-–—to]\", \"\", regex=True).str.replace(\",\", \"\", regex=False)\n",
    "    parts = txt.str.split(rf\"\\s*{RANGE_SEPS}\\s*\", n=1, regex=True)\n",
    "    pmin = parts.apply(lambda x: x[0] if isinstance(x, list) and len(x) >= 1 else np.nan)\n",
    "    pmax = parts.apply(lambda x: x[1] if isinstance(x, list) and len(x) >= 2 else np.nan)\n",
    "    pmin = pd.to_numeric(pmin, errors=\"coerce\")\n",
    "    pmax = pd.to_numeric(pmax, errors=\"coerce\")\n",
    "    price = pmin.copy()\n",
    "    both = pmin.notna() & pmax.notna()\n",
    "    price[both] = (pmin[both] + pmax[both]) / 2.0\n",
    "    return price\n",
    "\n",
    "def infer_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "for fname in FILES:\n",
    "    df = pd.read_csv(DATA_DIR / fname)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    price_col = infer_col(df, [\"price\",\"sale_price\",\"current_price\",\"list_price\",\"final_price\",\"price_usd\"])\n",
    "    brand_col = infer_col(df, [\"brand\",\"brand_name\",\"vendor\",\"label\"])\n",
    "\n",
    "    if price_col:\n",
    "        price_num = parse_price_series(df[price_col])\n",
    "        avg_price = round(price_num.mean(skipna=True), 2) if price_num.notna().any() else None\n",
    "    else:\n",
    "        avg_price = None\n",
    "\n",
    "    if brand_col:\n",
    "        brands = sorted(set(str(x) for x in df[brand_col].dropna() if str(x).strip()))\n",
    "        brands_str = \", \".join(brands)\n",
    "    else:\n",
    "        brands_str = \"—\"\n",
    "\n",
    "    rows.append({\n",
    "        \"File\": fname,\n",
    "        \"Product Categories\": CURATED_CATS.get(fname,\"—\"),\n",
    "        \"Avg Price\": f\"${avg_price:.2f}\" if isinstance(avg_price,(int,float)) else \"—\",\n",
    "        \"Brands\": brands_str\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b55051b-7de1-4e3a-a02a-725bf8b3da4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price(USD)</th>\n",
       "      <th>image_url</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_name</th>\n",
       "      <th>current_color_from_name</th>\n",
       "      <th>available_colors_cleaned</th>\n",
       "      <th>source_file</th>\n",
       "      <th>product_id</th>\n",
       "      <th>wishlist_pid</th>\n",
       "      <th>name</th>\n",
       "      <th>...</th>\n",
       "      <th>discount</th>\n",
       "      <th>colors</th>\n",
       "      <th>fit</th>\n",
       "      <th>color</th>\n",
       "      <th>variant</th>\n",
       "      <th>available_colors</th>\n",
       "      <th>title</th>\n",
       "      <th>price (USD)</th>\n",
       "      <th>stock_status</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.0</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w9679r-airlif...</td>\n",
       "      <td>Airlift Intrigue Bra</td>\n",
       "      <td>Espresso</td>\n",
       "      <td>Espresso, Espresso, Black, Navy, Anthracite, T...</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128.0</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w51314r-7-8-h...</td>\n",
       "      <td>7/8 High-Waist Airlift Legging</td>\n",
       "      <td>Espresso</td>\n",
       "      <td>Espresso, Espresso, Black, Navy, Anthracite, P...</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.0</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w6503r-work-i...</td>\n",
       "      <td>Work It Mini Skirt</td>\n",
       "      <td>Green Olive</td>\n",
       "      <td>Green Olive, Green Olive, Black</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228.0</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w4689r-work-i...</td>\n",
       "      <td>Work It Bomber</td>\n",
       "      <td>Green Olive</td>\n",
       "      <td>Green Olive, Green Olive, Black</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.0</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2185/2813/fi...</td>\n",
       "      <td>https://www.aloyoga.com/products/w9679r-airlif...</td>\n",
       "      <td>Airlift Intrigue Bra</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black, Black, Espresso, Navy, Anthracite, Toas...</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    price(USD)                                           image_url  \\\n",
       "0          68.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "1         128.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "2          98.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "3         228.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "4          68.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
       "\n",
       "                                         product_url  \\\n",
       "0  https://www.aloyoga.com/products/w9679r-airlif...   \n",
       "1  https://www.aloyoga.com/products/w51314r-7-8-h...   \n",
       "2  https://www.aloyoga.com/products/w6503r-work-i...   \n",
       "3  https://www.aloyoga.com/products/w4689r-work-i...   \n",
       "4  https://www.aloyoga.com/products/w9679r-airlif...   \n",
       "\n",
       "                     product_name current_color_from_name  \\\n",
       "0            Airlift Intrigue Bra                Espresso   \n",
       "1  7/8 High-Waist Airlift Legging                Espresso   \n",
       "2              Work It Mini Skirt             Green Olive   \n",
       "3                  Work It Bomber             Green Olive   \n",
       "4            Airlift Intrigue Bra                   Black   \n",
       "\n",
       "                            available_colors_cleaned  \\\n",
       "0  Espresso, Espresso, Black, Navy, Anthracite, T...   \n",
       "1  Espresso, Espresso, Black, Navy, Anthracite, P...   \n",
       "2                    Green Olive, Green Olive, Black   \n",
       "3                    Green Olive, Green Olive, Black   \n",
       "4  Black, Black, Espresso, Navy, Anthracite, Toas...   \n",
       "\n",
       "                     source_file product_id wishlist_pid name  ... discount  \\\n",
       "0  alo_yoga_products_cleaned.csv        NaN          NaN  NaN  ...      NaN   \n",
       "1  alo_yoga_products_cleaned.csv        NaN          NaN  NaN  ...      NaN   \n",
       "2  alo_yoga_products_cleaned.csv        NaN          NaN  NaN  ...      NaN   \n",
       "3  alo_yoga_products_cleaned.csv        NaN          NaN  NaN  ...      NaN   \n",
       "4  alo_yoga_products_cleaned.csv        NaN          NaN  NaN  ...      NaN   \n",
       "\n",
       "   colors  fit color variant available_colors title  price (USD)   \\\n",
       "0     NaN  NaN   NaN     NaN              NaN   NaN           NaN   \n",
       "1     NaN  NaN   NaN     NaN              NaN   NaN           NaN   \n",
       "2     NaN  NaN   NaN     NaN              NaN   NaN           NaN   \n",
       "3     NaN  NaN   NaN     NaN              NaN   NaN           NaN   \n",
       "4     NaN  NaN   NaN     NaN              NaN   NaN           NaN   \n",
       "\n",
       "  stock_status  sales  \n",
       "0          NaN    NaN  \n",
       "1          NaN    NaN  \n",
       "2          NaN    NaN  \n",
       "3          NaN    NaN  \n",
       "4          NaN    NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"./\")  # folder with your *_cleaned.csv files\n",
    "\n",
    "FILES = [\n",
    "    \"alo_yoga_products_cleaned.csv\",\n",
    "    \"altardstate_products_cleaned.csv\",\n",
    "    \"cupshe_products_cleaned.csv\",\n",
    "    \"edikted_products_cleaned.csv\",\n",
    "    \"gymshark_products_cleaned.csv\",\n",
    "    \"nakd_products_cleaned.csv\",\n",
    "    \"princess_polly_cleaned.csv\",\n",
    "    \"vuori_products_cleaned.csv\",\n",
    "]\n",
    "\n",
    "# stitch all into one dataframe with source_file column\n",
    "dfs = []\n",
    "for f in FILES:\n",
    "    df = pd.read_csv(DATA_DIR / f)\n",
    "    df[\"source_file\"] = f\n",
    "    dfs.append(df)\n",
    "all_cleaned = pd.concat(dfs, ignore_index=True)\n",
    "all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83bc7c1a-311c-44d5-9c39-497389dc06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CURATED_ALLOWED_CATEGORIES = {\n",
    "    \"alo_yoga_products_cleaned.csv\": [\"Bras\",\"Leggings\",\"Skirts\",\"Bombers\",\"Coverups\",\"Shorts\",\"Pullovers\",\"Sweatpants\",\"Tees\"],\n",
    "    \"altardstate_products_cleaned.csv\": [\"Dresses\",\"Maxi\",\"Midi\",\"Mini\"],\n",
    "    \"cupshe_products_cleaned.csv\": [\"Bikini Sets\",\"One-Piece Swimsuits\"],\n",
    "    \"edikted_products_cleaned.csv\": [\"Sweaters\",\"Sweatpants\",\"Jeans\",\"Tops\",\"Mini Skirts\",\"Shorts\",\"Skorts\"],\n",
    "    \"gymshark_products_cleaned.csv\": [\"Sports Bras\",\"Crop Tops\",\"Leggings\",\"Shorts\",\"One-Pieces\"],\n",
    "    \"nakd_products_cleaned.csv\": [\"Coats\",\"Jackets\",\"Blazers\",\"Jeans\",\"Tops\",\"Sweater\",\"Boots\",\"Ballerinas\",\"Trenchcoats\"],\n",
    "    \"princess_polly_cleaned.csv\": [\"Mini Dresses\"],\n",
    "    \"vuori_products_cleaned.csv\": [\"Pants\",\"Leggings\",\"Tees\",\"Joggers\",\"Tops\",\"Jackets\",\"Tank\",\"Cardigan\",\"Hoodie\",\"Trouser\",\"Bra\",\"Short\"],\n",
    "}\n",
    "\n",
    "CATEGORY_KEYWORDS = {\n",
    "    \"Bikini Sets\": [r\"\\bbikini\\b\"],\n",
    "    \"One-Piece Swimsuits\": [r\"\\bone[-\\s]?piece\\b\", r\"\\bswimsuit\\b\"],\n",
    "    \"Leggings\": [r\"\\blegging(s)?\\b\"],\n",
    "    \"Shorts\": [r\"\\bshort(s)?\\b\"],\n",
    "    \"Bras\": [r\"\\bbra(s)?\\b\"],\n",
    "    \"Sports Bras\": [r\"\\bsports?\\s*bra(s)?\\b\"],\n",
    "    \"Sweatpants\": [r\"\\bsweatpant(s)?\\b\"],\n",
    "    \"Sweaters\": [r\"\\bsweater(s)?\\b\"],\n",
    "    \"Dresses\": [r\"\\bdress(es)?\\b\"],\n",
    "    \"Mini Dresses\": [r\"\\bmini\\b.*\\bdress\\b\"],\n",
    "    \"Coats\": [r\"\\bcoat(s)?\\b\"],\n",
    "    \"Jackets\": [r\"\\bjacket(s)?\\b\"],\n",
    "    \"Tops\": [r\"\\btop(s)?\\b\", r\"\\btee(s)?\\b\", r\"\\btank\\b\"],\n",
    "    \"Jeans\": [r\"\\bjean(s)?\\b\"],\n",
    "    # add more as needed\n",
    "}\n",
    "\n",
    "compiled_rx = {cat: re.compile(\"|\".join(pats), flags=re.I) for cat, pats in CATEGORY_KEYWORDS.items()}\n",
    "\n",
    "def classify(row):\n",
    "    src = row[\"source_file\"]\n",
    "    text = \" \".join([str(row.get(\"product_name\",\"\")), str(row.get(\"category\",\"\"))]).lower()\n",
    "    allowed = CURATED_ALLOWED_CATEGORIES.get(src, CATEGORY_KEYWORDS.keys())\n",
    "    for cat in allowed:\n",
    "        if cat in compiled_rx and compiled_rx[cat].search(text):\n",
    "            return cat\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "all_cleaned[\"final_category\"] = all_cleaned.apply(classify, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c28113d-cdb1-4bba-96d6-25bcbde0eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPS = {\n",
    "    \"Swimwear\": [\"Bikini Sets\",\"One-Piece Swimsuits\"],\n",
    "    \"Activewear\": [\"Leggings\",\"Sports Bras\",\"Shorts\",\"Sweatpants\",\"Bras\"],\n",
    "    \"Outerwear\": [\"Coats\",\"Jackets\"],\n",
    "    \"Dresses\": [\"Dresses\",\"Mini Dresses\"],\n",
    "    \"Other\": []\n",
    "}\n",
    "\n",
    "def map_group(cat):\n",
    "    for g,members in GROUPS.items():\n",
    "        if cat in members:\n",
    "            return g\n",
    "    return \"Other\"\n",
    "\n",
    "all_cleaned[\"category_group\"] = all_cleaned[\"final_category\"].apply(map_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab7592aa-9133-42fa-92d9-f0219042683a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: [PosixPath('by_category_groups/Dresses.csv'), PosixPath('by_category_groups/Other.csv'), PosixPath('by_category_groups/Activewear.csv')]\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"by_category_groups\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for grp, g in all_cleaned.groupby(\"category_group\"):\n",
    "    g.to_csv(out_dir / f\"{grp}.csv\", index=False)\n",
    "\n",
    "print(\"Exported:\", list(out_dir.glob(\"*.csv\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fea76cfc-ce2a-47ac-a7c8-69f308f4828a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Other         6883\n",
      "Activewear     416\n",
      "Outerwear       52\n",
      "Dresses         24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "DATA_DIR = Path(\"./\")  # where your *_cleaned.csv files are\n",
    "FILES = [\n",
    "    \"alo_yoga_products_cleaned.csv\",\n",
    "    \"altardstate_products_cleaned.csv\",\n",
    "    \"cupshe_products_cleaned.csv\",\n",
    "    \"edikted_products_cleaned.csv\",\n",
    "    \"gymshark_products_cleaned.csv\",\n",
    "    \"nakd_products_cleaned.csv\",\n",
    "    \"princess_polly_cleaned.csv\",\n",
    "    \"vuori_products_cleaned.csv\",\n",
    "]\n",
    "\n",
    "# map filename → brand name (fallback if brand col missing)\n",
    "FILE_BRANDS = {\n",
    "    \"alo_yoga_products_cleaned.csv\": \"Alo Yoga\",\n",
    "    \"altardstate_products_cleaned.csv\": \"Altar'd State\",\n",
    "    \"cupshe_products_cleaned.csv\": \"Cupshe\",\n",
    "    \"edikted_products_cleaned.csv\": \"Edikted\",\n",
    "    \"gymshark_products_cleaned.csv\": \"Gymshark\",\n",
    "    \"nakd_products_cleaned.csv\": \"NA-KD\",\n",
    "    \"princess_polly_cleaned.csv\": \"Princess Polly\",\n",
    "    \"vuori_products_cleaned.csv\": \"Vuori\",\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for f in FILES:\n",
    "    df = pd.read_csv(DATA_DIR / f)\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "\n",
    "    # add missing brand\n",
    "    if \"brand\" not in df.columns:\n",
    "        df[\"brand\"] = FILE_BRANDS[f]\n",
    "\n",
    "    df[\"source_file\"] = f\n",
    "    dfs.append(df)\n",
    "\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# normalize key columns (fill missing)\n",
    "if \"product_name\" not in all_data.columns:\n",
    "    raise ValueError(\"No product_name column found — check cleaned files\")\n",
    "\n",
    "# ---------------------\n",
    "# Category classification\n",
    "# ---------------------\n",
    "CATEGORY_KEYWORDS = {\n",
    "    \"Swimwear\": [\"bikini\", \"swimsuit\", \"one-piece\"],\n",
    "    \"Activewear\": [\"legging\",\"bra\",\"sports bra\",\"short\",\"tee\",\"tank\",\"jogger\",\"hoodie\",\n",
    "                   \"pullover\",\"sweatpant\",\"crop top\",\"skort\",\"sweater\",\"cardigan\"],\n",
    "    \"Dresses\": [\"dress\",\"maxi\",\"midi\",\"mini\"],\n",
    "    \"Outerwear\": [\"jacket\",\"coat\",\"bomber\",\"blazer\",\"trench\"],\n",
    "    \"Footwear\": [\"boot\",\"ballerina\"],\n",
    "}\n",
    "\n",
    "def assign_group(row):\n",
    "    text = \" \".join([\n",
    "        str(row.get(\"product_name\",\"\")),\n",
    "        str(row.get(\"category\",\"\"))\n",
    "    ]).lower()\n",
    "    for group, kws in CATEGORY_KEYWORDS.items():\n",
    "        if any(re.search(rf\"\\b{k}\\b\", text) for k in kws):\n",
    "            return group\n",
    "    return \"Other\"\n",
    "\n",
    "all_data[\"category_group\"] = all_data.apply(assign_group, axis=1)\n",
    "\n",
    "# ---------------------\n",
    "# Save master\n",
    "# ---------------------\n",
    "all_data.to_csv(\"all_products_cleaned_master.csv\", index=False)\n",
    "\n",
    "print(all_data[\"category_group\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b09f2982-dc08-484a-bb48-9f3f0539fc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Other         6648\n",
      "Activewear     378\n",
      "Dresses        270\n",
      "Outerwear       79\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cropped Serenity Coverup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Suit Up Trouser (Regular)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ALO Softsculpt Precision 1/4 Zip Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ALO Sunset Sneaker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Unisex Half-Crew Throwback Sock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Serenity Wide Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ALO Sunset Sneaker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Pinstripe Daylight Boxer Pant (Regular)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ALO Runner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ALO Runner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Serenity Wide Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Off-Duty Cap</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Accolade Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Performance Off-Duty Cap</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Cropped Accolade Crewneck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Unisex Half-Crew Throwback Sock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Performance Conquer Headband</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Accolade Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>90's Claw Clip</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>ALO Runner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Scholar Knit High-Waist Cargo Pant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    product_name category  product_type  \\\n",
       "6                       Cropped Serenity Coverup      NaN           NaN   \n",
       "10               Accolade Straight Leg Sweatpant      NaN           NaN   \n",
       "13               Accolade Straight Leg Sweatpant      NaN           NaN   \n",
       "17                     Suit Up Trouser (Regular)      NaN           NaN   \n",
       "18  ALO Softsculpt Precision 1/4 Zip Long Sleeve      NaN           NaN   \n",
       "21                            ALO Sunset Sneaker      NaN           NaN   \n",
       "23               Unisex Half-Crew Throwback Sock      NaN           NaN   \n",
       "27               Accolade Straight Leg Sweatpant      NaN           NaN   \n",
       "28                   Serenity Wide Leg Sweatpant      NaN           NaN   \n",
       "29                            ALO Sunset Sneaker      NaN           NaN   \n",
       "31       Pinstripe Daylight Boxer Pant (Regular)      NaN           NaN   \n",
       "38                                    ALO Runner      NaN           NaN   \n",
       "43               Accolade Straight Leg Sweatpant      NaN           NaN   \n",
       "45                                    ALO Runner      NaN           NaN   \n",
       "47                   Serenity Wide Leg Sweatpant      NaN           NaN   \n",
       "48                                  Off-Duty Cap      NaN           NaN   \n",
       "57                            Accolade Sweatpant      NaN           NaN   \n",
       "59                      Performance Off-Duty Cap      NaN           NaN   \n",
       "60                     Cropped Accolade Crewneck      NaN           NaN   \n",
       "66               Unisex Half-Crew Throwback Sock      NaN           NaN   \n",
       "77                  Performance Conquer Headband      NaN           NaN   \n",
       "84                            Accolade Sweatpant      NaN           NaN   \n",
       "85                                90's Claw Clip      NaN           NaN   \n",
       "87                                    ALO Runner      NaN           NaN   \n",
       "90            Scholar Knit High-Waist Cargo Pant      NaN           NaN   \n",
       "\n",
       "       brand                    source_file  \n",
       "6   Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "10  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "13  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "17  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "18  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "21  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "23  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "27  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "28  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "29  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "31  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "38  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "43  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "45  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "47  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "48  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "57  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "59  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "60  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "66  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "77  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "84  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "85  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "87  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "90  Alo Yoga  alo_yoga_products_cleaned.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> all_products_cleaned_master_with_groups.csv\n"
     ]
    }
   ],
   "source": [
    "# === 1) Merge all cleaned CSVs ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"./\")\n",
    "FILES = [\n",
    "    \"alo_yoga_products_cleaned.csv\",\n",
    "    \"altardstate_products_cleaned.csv\",\n",
    "    \"cupshe_products_cleaned.csv\",\n",
    "    \"edikted_products_cleaned.csv\",\n",
    "    \"gymshark_products_cleaned.csv\",\n",
    "    \"nakd_products_cleaned.csv\",\n",
    "    \"princess_polly_cleaned.csv\",\n",
    "    \"vuori_products_cleaned.csv\",\n",
    "]\n",
    "\n",
    "FILE_BRANDS = {\n",
    "    \"alo_yoga_products_cleaned.csv\": \"Alo Yoga\",\n",
    "    \"altardstate_products_cleaned.csv\": \"ALTAR'D STATE\",\n",
    "    \"cupshe_products_cleaned.csv\": \"Cupshe\",\n",
    "    \"edikted_products_cleaned.csv\": \"Edikted\",\n",
    "    \"gymshark_products_cleaned.csv\": \"Gymshark\",\n",
    "    \"nakd_products_cleaned.csv\": \"NA-KD\",\n",
    "    \"princess_polly_cleaned.csv\": \"Princess Polly\",\n",
    "    \"vuori_products_cleaned.csv\": \"Vuori\",\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for f in FILES:\n",
    "    df = pd.read_csv(DATA_DIR / f)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    if \"brand\" not in df.columns:\n",
    "        df[\"brand\"] = FILE_BRANDS.get(f, \"\")\n",
    "    df[\"source_file\"] = f\n",
    "    dfs.append(df)\n",
    "\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Keep a tidy schema (add columns if present, else NaN)\n",
    "keep_cols = [\"product_name\",\"brand\",\"price\",\"category\",\"product_type\",\"url\",\"image_url\",\"sku\",\"id\",\"source_file\"]\n",
    "for c in keep_cols:\n",
    "    if c not in all_data.columns:\n",
    "        all_data[c] = np.nan\n",
    "\n",
    "# === 2) Build a rich searchable text field ===\n",
    "def join_text(row):\n",
    "    bits = [\n",
    "        str(row.get(\"product_name\",\"\")),\n",
    "        str(row.get(\"category\",\"\")),\n",
    "        str(row.get(\"product_type\",\"\"))\n",
    "    ]\n",
    "    return \" \".join([b for b in bits if b]).lower()\n",
    "\n",
    "all_data[\"_text\"] = all_data.apply(join_text, axis=1)\n",
    "\n",
    "# === 3) Robust keyword sets (regex with plural/variants) ===\n",
    "RX = {\n",
    "    \"Swimwear\": re.compile(\n",
    "        r\"\\b(bikini|bikini set[s]?|one[-\\s]?piece[s]?|swimsuit[s]?)\\b\", re.I\n",
    "    ),\n",
    "    \"Dresses\": re.compile(\n",
    "        r\"\\b(dress|dresses)\\b|\"\n",
    "        r\"\\b(mini|midi|maxi)\\b.*\\bdress(es)?\\b|\"\n",
    "        r\"\\bdress(es)?\\b.*\\b(mini|midi|maxi)\\b\", re.I\n",
    "    ),\n",
    "    \"Outerwear\": re.compile(\n",
    "        r\"\\b(jacket|jackets|coat|coats|bomber|bombers|blazer|blazers|trench|trenchcoat|trench coat|puffer|parka|windbreaker|shacket|gilet)\\b\",\n",
    "        re.I\n",
    "    ),\n",
    "    \"Footwear\": re.compile(\n",
    "        r\"\\b(boot|boots|ballerina|ballerinas|flat|flats)\\b\", re.I\n",
    "    ),\n",
    "    \"Activewear\": re.compile(\n",
    "        r\"\\b(legging|leggings|jogger|joggers|short|shorts|skirt|skirts|skort|skorts|\"\n",
    "        r\"tee|tees|t-?shirt|t-?shirts|tank|tanks|tank\\s*top|tank\\s*tops|\"\n",
    "        r\"bra|bras|sports?\\s*bra|sports?\\s*bras|\"\n",
    "        r\"hoodie|hoodies|sweater|sweaters|sweatshirt|sweatshirts|pullover|pullovers|\"\n",
    "        r\"cardigan|cardigans)\\b\",\n",
    "        re.I\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Priority so specific categories win before generic ones:\n",
    "PRIORITY = [\"Dresses\", \"Swimwear\", \"Outerwear\", \"Footwear\", \"Activewear\"]\n",
    "\n",
    "def classify_group(text: str) -> str:\n",
    "    for group in PRIORITY:\n",
    "        if RX[group].search(text or \"\"):\n",
    "            return group\n",
    "    return \"Other\"\n",
    "\n",
    "all_data[\"category_group\"] = all_data[\"_text\"].apply(classify_group)\n",
    "\n",
    "# === 4) Results & sanity checks ===\n",
    "counts = all_data[\"category_group\"].value_counts(dropna=False)\n",
    "print(counts)\n",
    "\n",
    "# peek at some \"Other\" rows to see what's being missed:\n",
    "others_sample = all_data.loc[all_data[\"category_group\"]==\"Other\", [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"]].head(25)\n",
    "display(others_sample)\n",
    "\n",
    "# save master with group\n",
    "all_data.drop(columns=[\"_text\"], inplace=True)\n",
    "all_data.to_csv(\"all_products_cleaned_master_with_groups.csv\", index=False)\n",
    "print(\"Saved -> all_products_cleaned_master_with_groups.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b00784b6-f851-4d11-8095-6f8c106dd90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Other         6518\n",
      "Activewear     479\n",
      "Dresses        270\n",
      "Outerwear       79\n",
      "Footwear        25\n",
      "Swimwear         4\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Scholar Knit High-Waist Cargo Pant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Notable Beanie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Fresh Mini Scrunchie (3-Pack)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Performance Fleece Gloves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Performance Fleece Beanie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Warrior Mat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>District Trucker Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Airlift Solar Visor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Ribbed Sea Coast Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Cotton Real Deal Button Up Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Pinstripe Daylight Button Down Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Alosoft High-Waist Iconic 90's Capri</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Cotton Real Deal Button Up Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Alosoft Finesse Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Breezy Wide Leg Pant (Regular)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Airlift Solar Visor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Dreamscape Button Down Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Semi-Sheer Delicate Lounge Flare Pant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Dreamscape Button Down Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Waffle Weekend Escape Mock Neck Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Airbrush One And Done Onesie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sherpa Recovery Slipper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Ribbed Stardust Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Airbrush Invisible Thong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Breezy Wide Leg Pant (Long)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Ribbed Modal Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Ribbed Stardust Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>District Trucker Hat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Alosoft Finesse Long Sleeve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Breezy Wide Leg Pant (Regular)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    product_name category  product_type  \\\n",
       "90            Scholar Knit High-Waist Cargo Pant      NaN           NaN   \n",
       "111                               Notable Beanie      NaN           NaN   \n",
       "114                Fresh Mini Scrunchie (3-Pack)      NaN           NaN   \n",
       "115                    Performance Fleece Gloves      NaN           NaN   \n",
       "116                    Performance Fleece Beanie      NaN           NaN   \n",
       "130                                  Warrior Mat      NaN           NaN   \n",
       "155                         District Trucker Hat      NaN           NaN   \n",
       "157                          Airlift Solar Visor      NaN           NaN   \n",
       "165                 Ribbed Sea Coast Long Sleeve      NaN           NaN   \n",
       "177       Cotton Real Deal Button Up Long Sleeve      NaN           NaN   \n",
       "178   Pinstripe Daylight Button Down Long Sleeve      NaN           NaN   \n",
       "179         Alosoft High-Waist Iconic 90's Capri      NaN           NaN   \n",
       "181       Cotton Real Deal Button Up Long Sleeve      NaN           NaN   \n",
       "184                  Alosoft Finesse Long Sleeve      NaN           NaN   \n",
       "185               Breezy Wide Leg Pant (Regular)      NaN           NaN   \n",
       "186                          Airlift Solar Visor      NaN           NaN   \n",
       "192           Dreamscape Button Down Long Sleeve      NaN           NaN   \n",
       "208        Semi-Sheer Delicate Lounge Flare Pant      NaN           NaN   \n",
       "212           Dreamscape Button Down Long Sleeve      NaN           NaN   \n",
       "222  Waffle Weekend Escape Mock Neck Long Sleeve      NaN           NaN   \n",
       "223                 Airbrush One And Done Onesie      NaN           NaN   \n",
       "226                      Sherpa Recovery Slipper      NaN           NaN   \n",
       "236                  Ribbed Stardust Long Sleeve      NaN           NaN   \n",
       "239                     Airbrush Invisible Thong      NaN           NaN   \n",
       "263                  Breezy Wide Leg Pant (Long)      NaN           NaN   \n",
       "264                     Ribbed Modal Long Sleeve      NaN           NaN   \n",
       "287                  Ribbed Stardust Long Sleeve      NaN           NaN   \n",
       "297                         District Trucker Hat      NaN           NaN   \n",
       "298                  Alosoft Finesse Long Sleeve      NaN           NaN   \n",
       "308               Breezy Wide Leg Pant (Regular)      NaN           NaN   \n",
       "\n",
       "        brand                    source_file  \n",
       "90   Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "111  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "114  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "115  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "116  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "130  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "155  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "157  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "165  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "177  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "178  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "179  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "181  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "184  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "185  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "186  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "192  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "208  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "212  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "222  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "223  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "226  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "236  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "239  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "263  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "264  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "287  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "297  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "298  Alo Yoga  alo_yoga_products_cleaned.csv  \n",
       "308  Alo Yoga  alo_yoga_products_cleaned.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: all_products_cleaned_master_with_groups.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Build a search text (if not already present)\n",
    "if \"_text\" not in all_data.columns:\n",
    "    def _join(r):\n",
    "        return \" \".join([\n",
    "            str(r.get(\"product_name\",\"\")),\n",
    "            str(r.get(\"category\",\"\")),\n",
    "            str(r.get(\"product_type\",\"\"))\n",
    "        ]).lower()\n",
    "    all_data[\"_text\"] = all_data.apply(_join, axis=1)\n",
    "\n",
    "# 2) Extend regex patterns to catch the \"Other\" cases you observed\n",
    "# NOTE: we only re-check rows currently labeled \"Other\"\n",
    "EXTRA_RX = {\n",
    "    \"Swimwear\": re.compile(r\"\\b(cover[-\\s]?up|coverup)s?\\b\", re.I),\n",
    "\n",
    "    \"Footwear\": re.compile(\n",
    "        r\"\\b(sneaker|sneakers|runner|runners|sock|socks)\\b\", re.I\n",
    "    ),\n",
    "\n",
    "    \"Activewear\": re.compile(\n",
    "        r\"\\b(\"\n",
    "        r\"sweatpant|sweatpants|\"\n",
    "        r\"trouser|trousers|\"\n",
    "        r\"zip\\s*long\\s*sleeve|1/4\\s*zip|quarter\\s*zip|\"\n",
    "        r\"crewneck|\"\n",
    "        r\"boxer\\s*pant|boxer\\s*pants|boxer\\s*short|boxer\\s*shorts|\"\n",
    "        r\"cap|caps|\"\n",
    "        r\"headband|headbands|\"\n",
    "        r\"claw\\s*clip|claw\\s*clips\"\n",
    "        r\")\\b\", re.I\n",
    "    ),\n",
    "\n",
    "    # If you also want to catch generic \"pant/pants\" as activewear, uncomment below:\n",
    "    # \"Activewear_generic\": re.compile(r\"\\b(pant|pants)\\b\", re.I),\n",
    "}\n",
    "\n",
    "# 3) Re-categorize only the rows in \"Other\" using the new patterns\n",
    "def recategorize_other(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"Other\"\n",
    "    if EXTRA_RX[\"Swimwear\"].search(text):\n",
    "        return \"Swimwear\"\n",
    "    if EXTRA_RX[\"Footwear\"].search(text):\n",
    "        return \"Footwear\"\n",
    "    if EXTRA_RX[\"Activewear\"].search(text):\n",
    "        return \"Activewear\"\n",
    "    return \"Other\"\n",
    "\n",
    "mask_other = all_data[\"category_group\"].eq(\"Other\")\n",
    "all_data.loc[mask_other, \"category_group\"] = all_data.loc[mask_other, \"_text\"].apply(recategorize_other)\n",
    "\n",
    "# 4) Check the new distribution and preview any remaining \"Other\"\n",
    "print(all_data[\"category_group\"].value_counts(dropna=False))\n",
    "\n",
    "others_left = all_data.loc[all_data[\"category_group\"].eq(\"Other\"),\n",
    "                           [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"]].head(30)\n",
    "display(others_left)\n",
    "\n",
    "# 5) (optional) Save updated master\n",
    "all_data.drop(columns=\"_text\", inplace=True, errors=\"ignore\")\n",
    "all_data.to_csv(\"all_products_cleaned_master_with_groups.csv\", index=False)\n",
    "print(\"Saved: all_products_cleaned_master_with_groups.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb9c7570-a993-4d7a-af69-221073ed4df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Other          6486\n",
      "Activewear      440\n",
      "Dresses         270\n",
      "Outerwear        82\n",
      "Accessories      72\n",
      "Footwear         21\n",
      "Swimwear          4\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Suit Up Trouser (Regular)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Serenity Wide Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Serenity Wide Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Accolade Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Accolade Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Suit Up Trouser (Regular)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Suit Up Trouser (Long)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>High-Waist Dreamscape Trouser (Regular)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Suit Up Trouser (Long)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>High-Waist Dreamscape Trouser (Long)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Sway Bootcut Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Accolade Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Accolade Straight Leg Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>High-Waist Pursuit Trouser (Regular)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Airbrush One And Done Onesie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Sherpa Recovery Slipper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Airbrush Invisible Thong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Accolade Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Accolade Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Make Waves Sweatpant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                product_name category  product_type     brand  \\\n",
       "10           Accolade Straight Leg Sweatpant      NaN           NaN  Alo Yoga   \n",
       "13           Accolade Straight Leg Sweatpant      NaN           NaN  Alo Yoga   \n",
       "17                 Suit Up Trouser (Regular)      NaN           NaN  Alo Yoga   \n",
       "27           Accolade Straight Leg Sweatpant      NaN           NaN  Alo Yoga   \n",
       "28               Serenity Wide Leg Sweatpant      NaN           NaN  Alo Yoga   \n",
       "43           Accolade Straight Leg Sweatpant      NaN           NaN  Alo Yoga   \n",
       "47               Serenity Wide Leg Sweatpant      NaN           NaN  Alo Yoga   \n",
       "57                        Accolade Sweatpant      NaN           NaN  Alo Yoga   \n",
       "84                        Accolade Sweatpant      NaN           NaN  Alo Yoga   \n",
       "131                Suit Up Trouser (Regular)      NaN           NaN  Alo Yoga   \n",
       "133                   Suit Up Trouser (Long)      NaN           NaN  Alo Yoga   \n",
       "137  High-Waist Dreamscape Trouser (Regular)      NaN           NaN  Alo Yoga   \n",
       "138                   Suit Up Trouser (Long)      NaN           NaN  Alo Yoga   \n",
       "159          Accolade Straight Leg Sweatpant      NaN           NaN  Alo Yoga   \n",
       "164     High-Waist Dreamscape Trouser (Long)      NaN           NaN  Alo Yoga   \n",
       "169                   Sway Bootcut Sweatpant      NaN           NaN  Alo Yoga   \n",
       "170                       Accolade Sweatpant      NaN           NaN  Alo Yoga   \n",
       "176          Accolade Straight Leg Sweatpant      NaN           NaN  Alo Yoga   \n",
       "194     High-Waist Pursuit Trouser (Regular)      NaN           NaN  Alo Yoga   \n",
       "223             Airbrush One And Done Onesie      NaN           NaN  Alo Yoga   \n",
       "226                  Sherpa Recovery Slipper      NaN           NaN  Alo Yoga   \n",
       "239                 Airbrush Invisible Thong      NaN           NaN  Alo Yoga   \n",
       "243                       Accolade Sweatpant      NaN           NaN  Alo Yoga   \n",
       "252                       Accolade Sweatpant      NaN           NaN  Alo Yoga   \n",
       "277                     Make Waves Sweatpant      NaN           NaN  Alo Yoga   \n",
       "\n",
       "                       source_file  \n",
       "10   alo_yoga_products_cleaned.csv  \n",
       "13   alo_yoga_products_cleaned.csv  \n",
       "17   alo_yoga_products_cleaned.csv  \n",
       "27   alo_yoga_products_cleaned.csv  \n",
       "28   alo_yoga_products_cleaned.csv  \n",
       "43   alo_yoga_products_cleaned.csv  \n",
       "47   alo_yoga_products_cleaned.csv  \n",
       "57   alo_yoga_products_cleaned.csv  \n",
       "84   alo_yoga_products_cleaned.csv  \n",
       "131  alo_yoga_products_cleaned.csv  \n",
       "133  alo_yoga_products_cleaned.csv  \n",
       "137  alo_yoga_products_cleaned.csv  \n",
       "138  alo_yoga_products_cleaned.csv  \n",
       "159  alo_yoga_products_cleaned.csv  \n",
       "164  alo_yoga_products_cleaned.csv  \n",
       "169  alo_yoga_products_cleaned.csv  \n",
       "170  alo_yoga_products_cleaned.csv  \n",
       "176  alo_yoga_products_cleaned.csv  \n",
       "194  alo_yoga_products_cleaned.csv  \n",
       "223  alo_yoga_products_cleaned.csv  \n",
       "226  alo_yoga_products_cleaned.csv  \n",
       "239  alo_yoga_products_cleaned.csv  \n",
       "243  alo_yoga_products_cleaned.csv  \n",
       "252  alo_yoga_products_cleaned.csv  \n",
       "277  alo_yoga_products_cleaned.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: all_products_cleaned_master_6groups.csv and by_category_groups_6\n"
     ]
    }
   ],
   "source": [
    "# --- Make sure we have a searchable text field ---\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if \"_text\" not in all_data.columns:\n",
    "    def _join(r):\n",
    "        return \" \".join([\n",
    "            str(r.get(\"product_name\",\"\")),\n",
    "            str(r.get(\"category\",\"\")),\n",
    "            str(r.get(\"product_type\",\"\"))\n",
    "        ]).lower()\n",
    "    all_data[\"_text\"] = all_data.apply(_join, axis=1)\n",
    "\n",
    "# --- 6-bucket regex (rich, plural-aware, hyphen-aware) ---\n",
    "RX = {\n",
    "    \"Dresses\": re.compile(\n",
    "        r\"\\b(dress|dresses)\\b|\"\n",
    "        r\"\\b(mini|midi|maxi)\\b.*\\bdress(es)?\\b|\"\n",
    "        r\"\\bdress(es)?\\b.*\\b(mini|midi|maxi)\\b\",\n",
    "        re.I\n",
    "    ),\n",
    "    \"Swimwear\": re.compile(\n",
    "        r\"\\b(bikini|bikini set[s]?|one[-\\s]?piece[s]?|swimsuit[s]?|cover[-\\s]?up|coverup|cover[-\\s]?ups|coverups)\\b\",\n",
    "        re.I\n",
    "    ),\n",
    "    \"Outerwear\": re.compile(\n",
    "        r\"\\b(jacket|jackets|coat|coats|bomber|bombers|blazer|blazers|\"\n",
    "        r\"trench|trench\\s*coat|trenchcoat|parka|puffer|windbreaker|\"\n",
    "        r\"shacket|gilet|vest|vests)\\b\",\n",
    "        re.I\n",
    "    ),\n",
    "    \"Footwear\": re.compile(\n",
    "        r\"\\b(boot|boots|sneaker|sneakers|runner|runners|ballerina|ballerinas|flat|flats|loafer|loafers|sandal|sandals)\\b\",\n",
    "        re.I\n",
    "    ),\n",
    "    \"Activewear\": re.compile(\n",
    "        r\"\\b(legging|leggings|jogger|joggers|pant|pants|capri|capris|\"\n",
    "        r\"short|shorts|skirt|skirts|skort|skorts|\"\n",
    "        r\"tee|tees|t-?shirt|t-?shirts|\"\n",
    "        r\"tank|tanks|long\\s*sleeve|crewneck|button\\s*down|henley|\"\n",
    "        r\"bra|bras|sports?\\s*bra|sports?\\s*bras|\"\n",
    "        r\"hoodie|hoodies|sweater|sweaters|sweatshirt|sweatshirts|pullover|pullovers|\"\n",
    "        r\"cardigan|cardigans)\\b\",\n",
    "        re.I\n",
    "    ),\n",
    "    \"Accessories\": re.compile(\n",
    "        r\"\\b(beanie|beanies|hat|hats|cap|caps|visor|visors|\"\n",
    "        r\"headband|headbands|scrunchie|scrunchies|\"\n",
    "        r\"glove|gloves|mittens?|scarf|scarves|belt|belts|\"\n",
    "        r\"clip|clips|claw\\s*clip|claw\\s*clips|\"\n",
    "        r\"mat|mats|yoga\\s*mat|\"\n",
    "        r\"bag|bags|tote|totes|backpack|backpacks|duffel|duffle|\"\n",
    "        r\"water\\s*bottle|bottle|towel|socks?)\\b\",\n",
    "        re.I\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Priority: put the *most specific apparel types* ahead of catch-all (Accessories)\n",
    "PRIORITY = [\"Dresses\", \"Swimwear\", \"Outerwear\", \"Footwear\", \"Activewear\", \"Accessories\"]\n",
    "\n",
    "def classify6(text: str) -> str:\n",
    "    text = text or \"\"\n",
    "    for grp in PRIORITY:\n",
    "        if RX[grp].search(text):\n",
    "            return grp\n",
    "    return \"Other\"\n",
    "\n",
    "all_data[\"category_group\"] = all_data[\"_text\"].apply(classify6)\n",
    "\n",
    "# --- Results & quick debug peek ---\n",
    "print(all_data[\"category_group\"].value_counts(dropna=False))\n",
    "\n",
    "others_sample = all_data.loc[all_data[\"category_group\"].eq(\"Other\"),\n",
    "                             [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"]].head(25)\n",
    "display(others_sample)\n",
    "\n",
    "# --- (optional) export per bucket ---\n",
    "out_dir = Path(\"by_category_groups_6\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "for grp, g in all_data.groupby(all_data[\"category_group\"].fillna(\"Other\")):\n",
    "    g.drop(columns=[\"_text\"], errors=\"ignore\").to_csv(out_dir / f\"{grp}.csv\", index=False)\n",
    "\n",
    "# Save full master with the 6-group label\n",
    "all_data.drop(columns=[\"_text\"], errors=\"ignore\").to_csv(\"all_products_cleaned_master_6groups.csv\", index=False)\n",
    "print(\"Saved:\", \"all_products_cleaned_master_6groups.csv\", \"and\", str(out_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3241bc0-0a34-48c2-a684-8e8efd44a0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Other          6663\n",
      "Activewear      508\n",
      "Outerwear        82\n",
      "Accessories      72\n",
      "Footwear         23\n",
      "Dresses          23\n",
      "Swimwear          4\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Untangled Hair Tie 6-Pack</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Unisex It Slide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>Unisex It Slide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Sheer Glow Boyshort</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>Unisex It Slide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>Cashmere Jet Set Crew</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>Sheer Glow Boyshort</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>Glow Wristband (2-Pack)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing/Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  product_name          category  product_type          brand  \\\n",
       "355  Untangled Hair Tie 6-Pack               NaN           NaN       Alo Yoga   \n",
       "395            Unisex It Slide               NaN           NaN       Alo Yoga   \n",
       "562            Unisex It Slide               NaN           NaN       Alo Yoga   \n",
       "566        Sheer Glow Boyshort               NaN           NaN       Alo Yoga   \n",
       "631            Unisex It Slide               NaN           NaN       Alo Yoga   \n",
       "661      Cashmere Jet Set Crew               NaN           NaN       Alo Yoga   \n",
       "670        Sheer Glow Boyshort               NaN           NaN       Alo Yoga   \n",
       "698    Glow Wristband (2-Pack)               NaN           NaN       Alo Yoga   \n",
       "720                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "721                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "722                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "723                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "724                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "725                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "726                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "727                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "728                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "729                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "730                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "731                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "732                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "733                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "734                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "735                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "736                        NaN  Clothing/Dresses           NaN  ALTAR'D STATE   \n",
       "\n",
       "                          source_file  \n",
       "355     alo_yoga_products_cleaned.csv  \n",
       "395     alo_yoga_products_cleaned.csv  \n",
       "562     alo_yoga_products_cleaned.csv  \n",
       "566     alo_yoga_products_cleaned.csv  \n",
       "631     alo_yoga_products_cleaned.csv  \n",
       "661     alo_yoga_products_cleaned.csv  \n",
       "670     alo_yoga_products_cleaned.csv  \n",
       "698     alo_yoga_products_cleaned.csv  \n",
       "720  altardstate_products_cleaned.csv  \n",
       "721  altardstate_products_cleaned.csv  \n",
       "722  altardstate_products_cleaned.csv  \n",
       "723  altardstate_products_cleaned.csv  \n",
       "724  altardstate_products_cleaned.csv  \n",
       "725  altardstate_products_cleaned.csv  \n",
       "726  altardstate_products_cleaned.csv  \n",
       "727  altardstate_products_cleaned.csv  \n",
       "728  altardstate_products_cleaned.csv  \n",
       "729  altardstate_products_cleaned.csv  \n",
       "730  altardstate_products_cleaned.csv  \n",
       "731  altardstate_products_cleaned.csv  \n",
       "732  altardstate_products_cleaned.csv  \n",
       "733  altardstate_products_cleaned.csv  \n",
       "734  altardstate_products_cleaned.csv  \n",
       "735  altardstate_products_cleaned.csv  \n",
       "736  altardstate_products_cleaned.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: all_products_cleaned_master_6groups.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re, unicodedata\n",
    "\n",
    "# --------- 1) Build a clean text field (fresh) ----------\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    # unicode normalize (fix curly quotes etc.)\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.lower()\n",
    "    # replace any non-letter/number with a space\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    # collapse multiple spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def row_text(r):\n",
    "    return normalize_text(\" \".join([\n",
    "        str(r.get(\"product_name\",\"\")),\n",
    "        str(r.get(\"category\",\"\")),\n",
    "        str(r.get(\"product_type\",\"\"))\n",
    "    ]))\n",
    "\n",
    "all_data[\"_t\"] = all_data.apply(row_text, axis=1)\n",
    "\n",
    "# --------- 2) Keyword sets (bigger net) ----------\n",
    "# Keep Accessories separate (your Option B)\n",
    "PAT = {\n",
    "    \"Dresses\": [\n",
    "        \"dress\", \"mini dress\", \"midi dress\", \"maxi dress\"\n",
    "    ],\n",
    "    \"Swimwear\": [\n",
    "        \"bikini\", \"one piece\", \"swimsuit\", \"coverup\", \"cover up\"\n",
    "    ],\n",
    "    \"Outerwear\": [\n",
    "        \"jacket\", \"coat\", \"bomber\", \"blazer\", \"trench\", \"parka\", \"puffer\",\n",
    "        \"windbreaker\", \"shacket\", \"gilet\", \"vest\"\n",
    "    ],\n",
    "    \"Footwear\": [\n",
    "        \"boot\", \"sneaker\", \"runner\", \"ballerina\", \"flat\", \"loafer\", \"sandal\", \"slipper\"\n",
    "    ],\n",
    "    \"Activewear\": [\n",
    "        # bottoms\n",
    "        \"legging\", \"jogger\", \"sweatpant\", \"pant\", \"trouser\", \"capri\", \"short\", \"skirt\", \"skort\",\n",
    "        # tops\n",
    "        \"tee\", \"t shirt\", \"tank\", \"long sleeve\", \"henley\", \"polo\", \"button down\", \"crewneck\",\n",
    "        # bras/sweaters\n",
    "        \"bra\", \"sports bra\", \"hoodie\", \"sweater\", \"sweatshirt\", \"pullover\", \"cardigan\",\n",
    "        # 1-piece garments often used athleisure\n",
    "        \"onesie\", \"jumpsuit\", \"romper\", \"unitard\", \"bodysuit\",\n",
    "        # under/innerwear commonly bundled as active\n",
    "        \"thong\", \"brief\", \"boxer\"\n",
    "    ],\n",
    "    \"Accessories\": [\n",
    "        \"beanie\", \"hat\", \"cap\", \"visor\", \"headband\", \"scrunchie\",\n",
    "        \"glove\", \"mitten\", \"scarf\", \"belt\", \"clip\", \"claw clip\",\n",
    "        \"mat\", \"yoga mat\", \"bag\", \"tote\", \"backpack\", \"duffel\", \"duffle\",\n",
    "        \"water bottle\", \"bottle\", \"towel\", \"sock\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Generate plural + simple variants automatically (e.g., \"pant\" -> \"pants\")\n",
    "def expand_terms(terms):\n",
    "    out = set()\n",
    "    for t in terms:\n",
    "        out.add(t)\n",
    "        if \" \" not in t:  # single word: add plurals\n",
    "            if not t.endswith(\"s\"): out.add(t + \"s\")\n",
    "        # also add hyphenless variants (t-shirt -> t shirt)\n",
    "        out.add(t.replace(\"-\", \" \"))\n",
    "    return sorted(out)\n",
    "\n",
    "PAT = {k: expand_terms(v) for k, v in PAT.items()}\n",
    "\n",
    "# Compile regex per group as OR of tokens (simple substring match over normalized text)\n",
    "RX = {k: re.compile(r\"(?:^| )(\" + \"|\".join(map(re.escape, v)) + r\")(?: |$)\") for k, v in PAT.items()}\n",
    "\n",
    "# priority so specific apparel wins before accessories\n",
    "PRIORITY = [\"Dresses\", \"Swimwear\", \"Outerwear\", \"Footwear\", \"Activewear\", \"Accessories\"]\n",
    "\n",
    "def classify(text: str) -> str:\n",
    "    for grp in PRIORITY:\n",
    "        if RX[grp].search(text):\n",
    "            return grp\n",
    "    return \"Other\"\n",
    "\n",
    "all_data[\"category_group\"] = all_data[\"_t\"].apply(classify)\n",
    "\n",
    "# --------- 3) Results + a peek at remaining Others ----------\n",
    "print(all_data[\"category_group\"].value_counts(dropna=False))\n",
    "display(all_data.loc[all_data[\"category_group\"].eq(\"Other\"),\n",
    "                     [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"]].head(25))\n",
    "\n",
    "# (optional) save\n",
    "all_data.drop(columns=[\"_t\"], inplace=True)\n",
    "all_data.to_csv(\"all_products_cleaned_master_6groups.csv\", index=False)\n",
    "print(\"Saved: all_products_cleaned_master_6groups.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf8cde96-fc57-4ffd-bc20-8cdf5f5fc753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Other          6409\n",
      "Activewear      510\n",
      "Dresses         270\n",
      "Outerwear        82\n",
      "Accessories      74\n",
      "Footwear         26\n",
      "Swimwear          4\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/4023112947.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Dresses' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  all_data.loc[mask, \"category_group\"] = grp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>Cashmere Jet Set Crew</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>altardstate_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              product_name  category  product_type          brand  \\\n",
       "661  Cashmere Jet Set Crew       NaN           NaN       Alo Yoga   \n",
       "967                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "968                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "969                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "970                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "971                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "972                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "973                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "974                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "975                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "976                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "977                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "978                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "979                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "980                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "981                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "982                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "983                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "984                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "985                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "986                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "987                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "988                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "989                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "990                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "991                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "992                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "993                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "994                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "995                    NaN  Clothing           NaN  ALTAR'D STATE   \n",
       "\n",
       "                          source_file  \n",
       "661     alo_yoga_products_cleaned.csv  \n",
       "967  altardstate_products_cleaned.csv  \n",
       "968  altardstate_products_cleaned.csv  \n",
       "969  altardstate_products_cleaned.csv  \n",
       "970  altardstate_products_cleaned.csv  \n",
       "971  altardstate_products_cleaned.csv  \n",
       "972  altardstate_products_cleaned.csv  \n",
       "973  altardstate_products_cleaned.csv  \n",
       "974  altardstate_products_cleaned.csv  \n",
       "975  altardstate_products_cleaned.csv  \n",
       "976  altardstate_products_cleaned.csv  \n",
       "977  altardstate_products_cleaned.csv  \n",
       "978  altardstate_products_cleaned.csv  \n",
       "979  altardstate_products_cleaned.csv  \n",
       "980  altardstate_products_cleaned.csv  \n",
       "981  altardstate_products_cleaned.csv  \n",
       "982  altardstate_products_cleaned.csv  \n",
       "983  altardstate_products_cleaned.csv  \n",
       "984  altardstate_products_cleaned.csv  \n",
       "985  altardstate_products_cleaned.csv  \n",
       "986  altardstate_products_cleaned.csv  \n",
       "987  altardstate_products_cleaned.csv  \n",
       "988  altardstate_products_cleaned.csv  \n",
       "989  altardstate_products_cleaned.csv  \n",
       "990  altardstate_products_cleaned.csv  \n",
       "991  altardstate_products_cleaned.csv  \n",
       "992  altardstate_products_cleaned.csv  \n",
       "993  altardstate_products_cleaned.csv  \n",
       "994  altardstate_products_cleaned.csv  \n",
       "995  altardstate_products_cleaned.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Stage 0: helpers & normalized fields ---\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def norm(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    # turn non-alphanum into space (so \"Clothing/Dresses\" -> \"clothing dresses\")\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def row_text(r):\n",
    "    return norm(\" \".join([\n",
    "        str(r.get(\"product_name\",\"\")),\n",
    "        str(r.get(\"category\",\"\")),\n",
    "        str(r.get(\"product_type\",\"\"))\n",
    "    ]))\n",
    "\n",
    "all_data[\"_cat_norm\"] = all_data[\"category\"].apply(norm) if \"category\" in all_data.columns else \"\"\n",
    "all_data[\"_text\"]     = all_data.apply(row_text, axis=1)\n",
    "\n",
    "# --- Stage 1: category-first mapping (deterministic) ---\n",
    "CAT_MAP = [\n",
    "    (\"Dresses\",   [\"dress\", \"dresses\"]),\n",
    "    (\"Swimwear\",  [\"bikini\", \"one piece\", \"swimsuit\", \"coverup\", \"cover up\"]),\n",
    "    (\"Outerwear\", [\"jacket\", \"coat\", \"blazer\", \"trench\", \"parka\", \"puffer\", \"windbreaker\", \"shacket\", \"gilet\", \"vest\"]),\n",
    "    (\"Footwear\",  [\"boot\", \"sneaker\", \"runner\", \"sandal\", \"slide\", \"loafer\", \"flat\", \"ballerina\"]),\n",
    "    # Activewear last here (it’s broader)\n",
    "    (\"Activewear\",[\"legging\", \"jogger\", \"sweatpant\", \"pant\", \"trouser\", \"capri\", \"short\", \"skirt\", \"skort\",\n",
    "                   \"tee\", \"t shirt\", \"tank\", \"long sleeve\", \"henley\", \"polo\", \"button down\", \"crewneck\",\n",
    "                   \"bra\", \"sports bra\", \"hoodie\", \"sweater\", \"sweatshirt\", \"pullover\", \"cardigan\",\n",
    "                   \"onesie\", \"jumpsuit\", \"romper\", \"unitard\", \"bodysuit\", \"thong\", \"brief\", \"boxer\"]),\n",
    "    (\"Accessories\",[\"beanie\", \"hat\", \"cap\", \"visor\", \"headband\", \"scrunchie\", \"glove\", \"mitten\", \"scarf\", \"belt\",\n",
    "                    \"clip\", \"claw clip\", \"wristband\", \"hair tie\", \"mat\", \"yoga mat\", \"bag\", \"tote\", \"backpack\",\n",
    "                    \"duffel\", \"duffle\", \"water bottle\", \"bottle\", \"towel\", \"sock\"]),\n",
    "]\n",
    "\n",
    "def contains_any(text, keys):\n",
    "    return any(f\" {k} \" in f\" {text} \" or f\" {k}s \" in f\" {text} \" for k in keys)\n",
    "\n",
    "# seed with NaN, fill from category first\n",
    "all_data[\"category_group\"] = np.nan\n",
    "for grp, keys in CAT_MAP:\n",
    "    mask = all_data[\"category_group\"].isna() & all_data[\"_cat_norm\"].apply(lambda t: contains_any(t, keys))\n",
    "    all_data.loc[mask, \"category_group\"] = grp\n",
    "\n",
    "# --- Stage 2: regex fallback on product_name + category + product_type ---\n",
    "PAT = {\n",
    "    \"Dresses\":     [\"dress\", \"mini dress\", \"midi dress\", \"maxi dress\"],\n",
    "    \"Swimwear\":    [\"bikini\", \"one piece\", \"swimsuit\", \"coverup\", \"cover up\"],\n",
    "    \"Outerwear\":   [\"jacket\", \"coat\", \"bomber\", \"blazer\", \"trench\", \"parka\", \"puffer\", \"windbreaker\", \"shacket\", \"gilet\", \"vest\"],\n",
    "    \"Footwear\":    [\"boot\", \"sneaker\", \"runner\", \"ballerina\", \"flat\", \"loafer\", \"sandal\", \"slide\", \"slipper\"],\n",
    "    \"Activewear\":  [\"legging\", \"jogger\", \"sweatpant\", \"pant\", \"trouser\", \"capri\", \"short\", \"skirt\", \"skort\",\n",
    "                    \"tee\", \"t shirt\", \"tank\", \"long sleeve\", \"henley\", \"polo\", \"button down\", \"crewneck\",\n",
    "                    \"bra\", \"sports bra\", \"hoodie\", \"sweater\", \"sweatshirt\", \"pullover\", \"cardigan\",\n",
    "                    \"onesie\", \"jumpsuit\", \"romper\", \"unitard\", \"bodysuit\", \"thong\", \"brief\", \"boxer\", \"boyshort\"],\n",
    "    \"Accessories\": [\"beanie\", \"hat\", \"cap\", \"visor\", \"headband\", \"scrunchie\", \"glove\", \"mitten\", \"scarf\", \"belt\",\n",
    "                    \"clip\", \"claw clip\", \"wristband\", \"hair tie\", \"mat\", \"yoga mat\", \"bag\", \"tote\", \"backpack\",\n",
    "                    \"duffel\", \"duffle\", \"water bottle\", \"bottle\", \"towel\", \"sock\", \"wrist band\"],\n",
    "}\n",
    "\n",
    "# auto-expand simple plurals and hyphenless\n",
    "def expand(terms):\n",
    "    out = set()\n",
    "    for t in terms:\n",
    "        out.add(t)\n",
    "        if \" \" not in t and not t.endswith(\"s\"): out.add(t + \"s\")\n",
    "        out.add(t.replace(\"-\", \" \"))\n",
    "    return sorted(out)\n",
    "\n",
    "RX = {g: re.compile(r\"(?:^| )(?:%s)(?: |$)\" % \"|\".join(map(re.escape, expand(v)))) for g, v in PAT.items()}\n",
    "PRIORITY = [\"Dresses\", \"Swimwear\", \"Outerwear\", \"Footwear\", \"Activewear\", \"Accessories\"]\n",
    "\n",
    "def classify_fallback(text):\n",
    "    for g in PRIORITY:\n",
    "        if RX[g].search(text):\n",
    "            return g\n",
    "    return np.nan\n",
    "\n",
    "mask_unlabeled = all_data[\"category_group\"].isna()\n",
    "all_data.loc[mask_unlabeled, \"category_group\"] = all_data.loc[mask_unlabeled, \"_text\"].apply(classify_fallback)\n",
    "\n",
    "# Final fill: anything still NA becomes Other\n",
    "all_data[\"category_group\"] = all_data[\"category_group\"].fillna(\"Other\")\n",
    "\n",
    "# Results\n",
    "print(all_data[\"category_group\"].value_counts(dropna=False))\n",
    "\n",
    "# Look at 30 remaining 'Other' examples to extend terms if needed\n",
    "display(all_data.loc[all_data[\"category_group\"].eq(\"Other\"),\n",
    "                     [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"]].head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57d843d9-03c2-4065-968a-6294e3c9dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Activewear     2523\n",
      "Dresses        2392\n",
      "Other          1968\n",
      "Outerwear       249\n",
      "Accessories     134\n",
      "Swimwear         76\n",
      "Footwear         33\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>alo_yoga_products_cleaned.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_name category  product_type     brand  \\\n",
       "0           NaN      NaN           NaN  Alo Yoga   \n",
       "1           NaN      NaN           NaN  Alo Yoga   \n",
       "2           NaN      NaN           NaN  Alo Yoga   \n",
       "3           NaN      NaN           NaN  Alo Yoga   \n",
       "4           NaN      NaN           NaN  Alo Yoga   \n",
       "5           NaN      NaN           NaN  Alo Yoga   \n",
       "6           NaN      NaN           NaN  Alo Yoga   \n",
       "7           NaN      NaN           NaN  Alo Yoga   \n",
       "8           NaN      NaN           NaN  Alo Yoga   \n",
       "9           NaN      NaN           NaN  Alo Yoga   \n",
       "10          NaN      NaN           NaN  Alo Yoga   \n",
       "11          NaN      NaN           NaN  Alo Yoga   \n",
       "12          NaN      NaN           NaN  Alo Yoga   \n",
       "13          NaN      NaN           NaN  Alo Yoga   \n",
       "14          NaN      NaN           NaN  Alo Yoga   \n",
       "15          NaN      NaN           NaN  Alo Yoga   \n",
       "16          NaN      NaN           NaN  Alo Yoga   \n",
       "17          NaN      NaN           NaN  Alo Yoga   \n",
       "18          NaN      NaN           NaN  Alo Yoga   \n",
       "19          NaN      NaN           NaN  Alo Yoga   \n",
       "\n",
       "                      source_file  \n",
       "0   alo_yoga_products_cleaned.csv  \n",
       "1   alo_yoga_products_cleaned.csv  \n",
       "2   alo_yoga_products_cleaned.csv  \n",
       "3   alo_yoga_products_cleaned.csv  \n",
       "4   alo_yoga_products_cleaned.csv  \n",
       "5   alo_yoga_products_cleaned.csv  \n",
       "6   alo_yoga_products_cleaned.csv  \n",
       "7   alo_yoga_products_cleaned.csv  \n",
       "8   alo_yoga_products_cleaned.csv  \n",
       "9   alo_yoga_products_cleaned.csv  \n",
       "10  alo_yoga_products_cleaned.csv  \n",
       "11  alo_yoga_products_cleaned.csv  \n",
       "12  alo_yoga_products_cleaned.csv  \n",
       "13  alo_yoga_products_cleaned.csv  \n",
       "14  alo_yoga_products_cleaned.csv  \n",
       "15  alo_yoga_products_cleaned.csv  \n",
       "16  alo_yoga_products_cleaned.csv  \n",
       "17  alo_yoga_products_cleaned.csv  \n",
       "18  alo_yoga_products_cleaned.csv  \n",
       "19  alo_yoga_products_cleaned.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === FIX & RECLASSIFY (6 buckets) ===\n",
    "import pandas as pd, numpy as np, re, unicodedata\n",
    "\n",
    "# 0) Harmonize product_name from alternatives and coalesce empties\n",
    "name_alts = [c for c in [\"product_name\",\"name\",\"title\"] if c in all_data.columns]\n",
    "if name_alts:\n",
    "    all_data[\"product_name\"] = None\n",
    "    for c in name_alts:\n",
    "        all_data[\"product_name\"] = all_data[\"product_name\"].fillna(all_data[c])\n",
    "# strip whitespace\n",
    "all_data[\"product_name\"] = all_data[\"product_name\"].astype(str).str.strip().replace({\"None\": np.nan, \"nan\": np.nan})\n",
    "\n",
    "# 1) Normalization helpers\n",
    "def norm(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)       # non-alnum -> space\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def row_text(r):\n",
    "    return norm(\" \".join([\n",
    "        str(r.get(\"product_name\",\"\")),\n",
    "        str(r.get(\"category\",\"\")),\n",
    "        str(r.get(\"product_type\",\"\")),\n",
    "    ]))\n",
    "\n",
    "all_data[\"_cat_norm\"] = all_data.get(\"category\", pd.Series(\"\", index=all_data.index)).apply(norm)\n",
    "all_data[\"_text\"]     = all_data.apply(row_text, axis=1)\n",
    "\n",
    "# 2) Build plural-aware regex quickly\n",
    "def plural_rx(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Turn 'dress' -> 'dress(?:es)?'; 'short' -> 'shorts?'; 'capri' -> 'capris?'\n",
    "    Simple rule that works for our vocab.\n",
    "    \"\"\"\n",
    "    if word.endswith(\"ss\"):   # dress -> dresses\n",
    "        return rf\"{re.escape(word)}(?:es)?\"\n",
    "    elif word.endswith(\"ch\") or word.endswith(\"sh\"):\n",
    "        return rf\"{re.escape(word)}(?:es)?\"\n",
    "    elif word.endswith(\"y\"):  # not handling consonant->ies here; we don't need it\n",
    "        return rf\"{re.escape(word)}s?\"\n",
    "    else:\n",
    "        return rf\"{re.escape(word)}s?\"\n",
    "\n",
    "def build_or_rx(words):\n",
    "    tokens = []\n",
    "    for w in words:\n",
    "        w = w.replace(\"-\", \" \")\n",
    "        if \" \" in w:\n",
    "            # phrases: just allow single space between tokens\n",
    "            tokens.append(re.escape(w))\n",
    "        else:\n",
    "            tokens.append(plural_rx(w))\n",
    "    return re.compile(rf\"(?:^|\\s)(?:{'|'.join(tokens)})(?:\\s|$)\", re.I)\n",
    "\n",
    "# 3) Stage-1: category-first deterministic mapping\n",
    "CAT_RULES = {\n",
    "    \"Dresses\":   [\"dress\"],  # matches dress/dresses\n",
    "    \"Swimwear\":  [\"bikini\", \"one piece\", \"swimsuit\", \"coverup\", \"cover up\"],\n",
    "    \"Outerwear\": [\"jacket\",\"coat\",\"bomber\",\"blazer\",\"trench\",\"parka\",\"puffer\",\"windbreaker\",\"shacket\",\"gilet\",\"vest\"],\n",
    "    \"Footwear\":  [\"boot\",\"sneaker\",\"runner\",\"sandal\",\"slide\",\"loafer\",\"flat\",\"ballerina\"],\n",
    "    \"Activewear\":[\"legging\",\"jogger\",\"sweatpant\",\"pant\",\"trouser\",\"capri\",\"short\",\"skirt\",\"skort\",\n",
    "                  \"tee\",\"t shirt\",\"tank\",\"long sleeve\",\"henley\",\"polo\",\"button down\",\"crewneck\",\n",
    "                  \"bra\",\"sports bra\",\"hoodie\",\"sweater\",\"sweatshirt\",\"pullover\",\"cardigan\",\n",
    "                  \"onesie\",\"jumpsuit\",\"romper\",\"unitard\",\"bodysuit\",\"thong\",\"brief\",\"boxer\",\"boyshort\"],\n",
    "    \"Accessories\":[\"beanie\",\"hat\",\"cap\",\"visor\",\"headband\",\"scrunchie\",\"glove\",\"mitten\",\"scarf\",\"belt\",\n",
    "                   \"clip\",\"claw clip\",\"wristband\",\"hair tie\",\"mat\",\"yoga mat\",\"bag\",\"tote\",\"backpack\",\n",
    "                   \"duffel\",\"duffle\",\"water bottle\",\"bottle\",\"towel\",\"sock\"],\n",
    "}\n",
    "CAT_RX = {k: build_or_rx(v) for k,v in CAT_RULES.items()}\n",
    "\n",
    "# make sure target column accepts strings\n",
    "all_data[\"category_group\"] = pd.Series(pd.NA, index=all_data.index, dtype=\"object\")\n",
    "\n",
    "for grp in [\"Dresses\",\"Swimwear\",\"Outerwear\",\"Footwear\",\"Activewear\",\"Accessories\"]:\n",
    "    mask = all_data[\"category_group\"].isna() & all_data[\"_cat_norm\"].str.len().gt(0) & all_data[\"_cat_norm\"].apply(lambda t: bool(CAT_RX[grp].search(t)))\n",
    "    all_data.loc[mask, \"category_group\"] = grp\n",
    "\n",
    "# 4) Stage-2: fallback on product_name + category + product_type combined text\n",
    "FALLBACK_RULES = CAT_RULES  # reuse vocab\n",
    "FB_RX = {k: build_or_rx(v) for k,v in FALLBACK_RULES.items()}\n",
    "PRIORITY = [\"Dresses\",\"Swimwear\",\"Outerwear\",\"Footwear\",\"Activewear\",\"Accessories\"]\n",
    "\n",
    "def fallback_label(t: str):\n",
    "    for grp in PRIORITY:\n",
    "        if FB_RX[grp].search(t):\n",
    "            return grp\n",
    "    return pd.NA\n",
    "\n",
    "mask_unlabeled = all_data[\"category_group\"].isna()\n",
    "all_data.loc[mask_unlabeled, \"category_group\"] = all_data.loc[mask_unlabeled, \"_text\"].apply(fallback_label)\n",
    "\n",
    "# final fill\n",
    "all_data[\"category_group\"] = all_data[\"category_group\"].fillna(\"Other\")\n",
    "\n",
    "print(all_data[\"category_group\"].value_counts(dropna=False).sort_values(ascending=False).to_string())\n",
    "display(all_data.loc[all_data[\"category_group\"].eq(\"Other\"), [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"]].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cca59a84-ddb8-4905-89bc-4ce77fe450c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' price(USD) ', 'image_url', 'product_url', 'product_name', 'current_color_from_name', 'available_colors_cleaned']\n",
      "    price(USD)                                           image_url  \\\n",
      "0          68.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "1         128.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "2          98.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "3         228.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "4          68.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "5         128.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "6         118.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "7          68.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "8          58.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "9         128.0  https://cdn.shopify.com/s/files/1/2185/2813/fi...   \n",
      "\n",
      "                                         product_url  \\\n",
      "0  https://www.aloyoga.com/products/w9679r-airlif...   \n",
      "1  https://www.aloyoga.com/products/w51314r-7-8-h...   \n",
      "2  https://www.aloyoga.com/products/w6503r-work-i...   \n",
      "3  https://www.aloyoga.com/products/w4689r-work-i...   \n",
      "4  https://www.aloyoga.com/products/w9679r-airlif...   \n",
      "5  https://www.aloyoga.com/products/w5766r-7-8-hi...   \n",
      "6  https://www.aloyoga.com/products/w3875r-croppe...   \n",
      "7  https://www.aloyoga.com/products/w6472rg-accol...   \n",
      "8  https://www.aloyoga.com/products/w1445r-ribbed...   \n",
      "9  https://www.aloyoga.com/products/u3031rg-accol...   \n",
      "\n",
      "                                product_name current_color_from_name  \\\n",
      "0                       Airlift Intrigue Bra                Espresso   \n",
      "1             7/8 High-Waist Airlift Legging                Espresso   \n",
      "2                         Work It Mini Skirt             Green Olive   \n",
      "3                             Work It Bomber             Green Olive   \n",
      "4                       Airlift Intrigue Bra                   Black   \n",
      "5             7/8 High-Waist Airlift Legging                   Black   \n",
      "6                   Cropped Serenity Coverup                   Ivory   \n",
      "7                             Accolade Short                   Ivory   \n",
      "8  Ribbed Sea Coast Cropped Short Sleeve Tee                   Ivory   \n",
      "9                Accolade Crew Neck Pullover                Espresso   \n",
      "\n",
      "                            available_colors_cleaned  \n",
      "0  Espresso, Espresso, Black, Navy, Anthracite, T...  \n",
      "1  Espresso, Espresso, Black, Navy, Anthracite, P...  \n",
      "2                    Green Olive, Green Olive, Black  \n",
      "3                    Green Olive, Green Olive, Black  \n",
      "4  Black, Black, Espresso, Navy, Anthracite, Toas...  \n",
      "5  Black, Black, Espresso, Navy, Anthracite, Pink...  \n",
      "6         Ivory, Ivory, Black, Athletic Heather Grey  \n",
      "7  Ivory, Ivory, Black, Athletic Heather Grey, Na...  \n",
      "8                                Ivory, Ivory, Black  \n",
      "9  Espresso, Espresso, Black, Athletic Heather Gr...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_alo = pd.read_csv(\"alo_yoga_products_cleaned.csv\")\n",
    "print(df_alo.columns.tolist())\n",
    "print(df_alo.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11679041-17cb-4b82-b386-eceae294f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alo columns: [' price(USD) ', 'image_url', 'product_url', 'product_name', 'current_color_from_name', 'available_colors_cleaned']\n",
      "product_name nulls: 0\n",
      "product_name examples: ['Airlift Intrigue Bra', '7/8 High-Waist Airlift Legging', 'Work It Mini Skirt', 'Work It Bomber', 'Airlift Intrigue Bra', '7/8 High-Waist Airlift Legging', 'Cropped Serenity Coverup', 'Accolade Short', 'Ribbed Sea Coast Cropped Short Sleeve Tee', 'Accolade Crew Neck Pullover']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_alo = pd.read_csv(\"alo_yoga_products_cleaned.csv\")\n",
    "\n",
    "print(\"Alo columns:\", list(df_alo.columns))\n",
    "print(\"product_name nulls:\", df_alo[\"product_name\"].isna().sum())\n",
    "print(\"product_name examples:\", df_alo[\"product_name\"].dropna().head(10).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d2f3060-0e5f-4107-af69-5ac235a4d3c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m        df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m FILE_BRANDS\u001b[38;5;241m.\u001b[39mget(fname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m    frames\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m--> 123\u001b[0m all_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(frames, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# -------------------------------------------\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# 2) SAFE coalesce for product_name (no wipe)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# -------------------------------------------\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_data\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    " # === Merge + Safe product_name + 6-Group Categorization (one cell) ===\n",
    "import pandas as pd, numpy as np, re, unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------\n",
    "# CONFIG: paths & filenames\n",
    "# --------------------------\n",
    "DATA_DIR = Path(\"./\")  # folder containing your *_cleaned.csv files\n",
    "FILES = [\n",
    "    \"alo_yoga_products_cleaned.csv\",\n",
    "    \"altardstate_products_cleaned.csv\",\n",
    "    \"cupshe_products_cleaned.csv\",\n",
    "    \"edikted_products_cleaned.csv\",\n",
    "    \"gymshark_products_cleaned.csv\",\n",
    "    \"nakd_products_cleaned.csv\",\n",
    "    \"princess_polly_cleaned.csv\",\n",
    "    \"vuori_products_cleaned.csv\",\n",
    "]\n",
    "FILE_BRANDS = {\n",
    "    \"alo_yoga_products_cleaned.csv\": \"Alo Yoga\",\n",
    "    \"altardstate_products_cleaned.csv\": \"ALTAR'D STATE\",\n",
    "    \"cupshe_products_cleaned.csv\": \"Cupshe\",\n",
    "    \"edikted_products_cleaned.csv\": \"Edikted\",\n",
    "    \"gymshark_products_cleaned.csv\": \"Gymshark\",\n",
    "    \"nakd_products_cleaned.csv\": \"NA-KD\",\n",
    "    \"princess_polly_cleaned.csv\": \"Princess Polly\",\n",
    "    \"vuori_products_cleaned.csv\": \"Vuori\",\n",
    "}\n",
    "OUT_MASTER = \"all_products_cleaned_master_6groups.csv\"\n",
    "OUT_DIR_BUCKETS = Path(\"by_category_groups_6\")\n",
    "OUT_DIR_BUCKETS.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------------------------------\n",
    "# Helpers: column canon & text norm\n",
    "# ---------------------------------\n",
    "def canon_colname(c: str) -> str:\n",
    "    raw = unicodedata.normalize(\"NFKC\", str(c)).strip().lower()\n",
    "    # common cleanups\n",
    "    raw = raw.replace(\"/\", \" \").replace(\"\\\\\", \" \")\n",
    "    raw = re.sub(r\"\\s+\", \" \", raw)\n",
    "    # parenthesis to underscores\n",
    "    tmp = raw.replace(\"(\", \"_\").replace(\")\", \"_\")\n",
    "    tmp = re.sub(r\"[^a-z0-9]+\", \"_\", tmp).strip(\"_\")\n",
    "    # map common variants\n",
    "    aliases = {\n",
    "        \"productname\": \"product_name\",\n",
    "        \"product_name\": \"product_name\",\n",
    "        \"name\": \"product_name\",\n",
    "        \"title\": \"product_name\",\n",
    "        \"price\": \"price\",\n",
    "        \"price_usd\": \"price\",\n",
    "        \"priceusd\": \"price\",\n",
    "        \"price_us\": \"price\",\n",
    "        \"price_usd_\": \"price\",\n",
    "        \"price_usd__\": \"price\",\n",
    "        \"image_url\": \"image_url\",\n",
    "        \"product_url\": \"url\",\n",
    "        \"url\": \"url\",\n",
    "        \"link\": \"url\",\n",
    "        \"category\": \"category\",\n",
    "        \"product_category\": \"category\",\n",
    "        \"product_type\": \"product_type\",\n",
    "        \"type\": \"product_type\",\n",
    "        \"categories\": \"category\",\n",
    "        \"taxonomy\": \"category\",\n",
    "        \"sku\": \"sku\",\n",
    "        \"id\": \"sku\",\n",
    "        \"product_id\": \"sku\",\n",
    "    }\n",
    "    # special case like \"price_usd\" embedded\n",
    "    if \"price_usd\" in tmp or raw.strip() == \"price(usd)\":\n",
    "        return \"price\"\n",
    "    return aliases.get(tmp, tmp)\n",
    "\n",
    "def norm_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "# --------------------------\n",
    "# 1) Read & merge all files\n",
    "# --------------------------\n",
    "frames = []\n",
    "for fname in FILES:\n",
    "    df = pd.read_csv(DATA_DIR / fname)\n",
    "    df.columns = [canon_colname(c) for c in df.columns]\n",
    "    # after: df = pd.read_csv(DATA_DIR / fname)\n",
    "# after: df.columns = [canon_colname(c) for c in df.columns]\n",
    "\n",
    "# --- ensure unique columns by coalescing duplicates ---\n",
    "if df.columns.duplicated().any():\n",
    "    # build a new frame with one column per name\n",
    "    cols = df.columns.tolist()\n",
    "    from collections import defaultdict\n",
    "    groups = defaultdict(list)\n",
    "    for i, c in enumerate(cols):\n",
    "        groups[c].append(i)\n",
    "\n",
    "    new_df = pd.DataFrame(index=df.index)\n",
    "    for c, idxs in groups.items():\n",
    "        if len(idxs) == 1:\n",
    "            new_df[c] = df.iloc[:, idxs[0]]\n",
    "        else:\n",
    "            # coalesce left->right: take first non-empty\n",
    "            s = df.iloc[:, idxs[0]]\n",
    "            for j in idxs[1:]:\n",
    "                cand = df.iloc[:, j]\n",
    "                # treat \"\", \"nan\", \"None\" as empty\n",
    "                empty = s.isna() | (s.astype(str).str.strip().isin([\"\", \"nan\", \"None\"]))\n",
    "                s = s.where(~empty, cand)\n",
    "            new_df[c] = s\n",
    "    df = new_df\n",
    "# -----------------------------------------------\n",
    "\n",
    "    df[\"source_file\"] = fname\n",
    "    # brand fallback from filename if needed\n",
    "    if \"brand\" not in df.columns or df[\"brand\"].isna().all():\n",
    "        df[\"brand\"] = FILE_BRANDS.get(fname, \"\")\n",
    "    frames.append(df)\n",
    "\n",
    "all_data = pd.concat(frames, ignore_index=True, sort=False)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2) SAFE coalesce for product_name (no wipe)\n",
    "# -------------------------------------------\n",
    "if \"product_name\" not in all_data.columns:\n",
    "    all_data[\"product_name\"] = pd.NA\n",
    "\n",
    "for alt in [\"name\", \"title\"]:\n",
    "    if alt in all_data.columns:\n",
    "        mask = all_data[\"product_name\"].isna() | (all_data[\"product_name\"].astype(str).str.strip() == \"\")\n",
    "        all_data.loc[mask, \"product_name\"] = all_data.loc[mask, alt]\n",
    "\n",
    "# last-resort: derive from URL slug if still missing\n",
    "if \"url\" in all_data.columns:\n",
    "    mask = all_data[\"product_name\"].isna()\n",
    "    all_data.loc[mask, \"product_name\"] = (\n",
    "        all_data.loc[mask, \"url\"]\n",
    "        .astype(str)\n",
    "        .str.extract(r\"/([^/?#]+)(?:\\?.*)?$\")[0]\n",
    "        .str.replace(\"-\", \" \", regex=False)\n",
    "        .str.replace(\"_\", \" \", regex=False)\n",
    "    )\n",
    "\n",
    "# clean empties to NaN\n",
    "all_data[\"product_name\"] = (\n",
    "    all_data[\"product_name\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace({\"\": pd.NA, \"nan\": pd.NA, \"None\": pd.NA})\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Build normalized text fields\n",
    "# -------------------------------\n",
    "all_data[\"_cat_norm\"] = all_data.get(\"category\", pd.Series(\"\", index=all_data.index)).apply(norm_text)\n",
    "all_data[\"_text\"] = (\n",
    "    all_data.get(\"product_name\", pd.Series(\"\", index=all_data.index)).apply(norm_text)\n",
    "    + \" \" + all_data.get(\"category\", pd.Series(\"\", index=all_data.index)).apply(norm_text)\n",
    "    + \" \" + all_data.get(\"product_type\", pd.Series(\"\", index=all_data.index)).apply(norm_text)\n",
    ").str.strip()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Stage-1: category-first labeling (deterministic)\n",
    "# -------------------------------------------------\n",
    "def plural_rx(word: str) -> str:\n",
    "    # very simple pluralization that covers our vocab well\n",
    "    if word.endswith((\"ss\", \"ch\", \"sh\")):\n",
    "        return rf\"{re.escape(word)}(?:es)?\"\n",
    "    elif word.endswith(\"y\"):\n",
    "        return rf\"{re.escape(word)}s?\"   # good enough for our tokens\n",
    "    else:\n",
    "        return rf\"{re.escape(word)}s?\"\n",
    "\n",
    "def build_or_rx(words):\n",
    "    tokens = []\n",
    "    for w in words:\n",
    "        w = w.replace(\"-\", \" \")\n",
    "        if \" \" in w:  # phrases\n",
    "            tokens.append(re.escape(w))\n",
    "        else:\n",
    "            tokens.append(plural_rx(w))\n",
    "    return re.compile(rf\"(?:^|\\s)(?:{'|'.join(tokens)})(?:\\s|$)\", re.I)\n",
    "\n",
    "CAT_RULES = {\n",
    "    \"Dresses\":   [\"dress\"],\n",
    "    \"Swimwear\":  [\"bikini\", \"one piece\", \"swimsuit\", \"coverup\", \"cover up\"],\n",
    "    \"Outerwear\": [\"jacket\",\"coat\",\"bomber\",\"blazer\",\"trench\",\"parka\",\"puffer\",\"windbreaker\",\"shacket\",\"gilet\",\"vest\"],\n",
    "    \"Footwear\":  [\"boot\",\"sneaker\",\"runner\",\"sandal\",\"slide\",\"loafer\",\"flat\",\"ballerina\",\"slipper\"],\n",
    "    \"Activewear\":[\"legging\",\"jogger\",\"sweatpant\",\"pant\",\"trouser\",\"capri\",\"short\",\"skirt\",\"skort\",\n",
    "                  \"tee\",\"t shirt\",\"tank\",\"long sleeve\",\"henley\",\"polo\",\"button down\",\"crewneck\",\n",
    "                  \"bra\",\"sports bra\",\"hoodie\",\"sweater\",\"sweatshirt\",\"pullover\",\"cardigan\",\n",
    "                  \"onesie\",\"jumpsuit\",\"romper\",\"unitard\",\"bodysuit\",\"thong\",\"brief\",\"boxer\",\"boyshort\"],\n",
    "    \"Accessories\":[\"beanie\",\"hat\",\"cap\",\"visor\",\"headband\",\"scrunchie\",\"glove\",\"mitten\",\"scarf\",\"belt\",\n",
    "                   \"clip\",\"claw clip\",\"wristband\",\"hair tie\",\"mat\",\"yoga mat\",\"bag\",\"tote\",\"backpack\",\n",
    "                   \"duffel\",\"duffle\",\"water bottle\",\"bottle\",\"towel\",\"sock\"],\n",
    "}\n",
    "CAT_RX = {k: build_or_rx(v) for k, v in CAT_RULES.items()}\n",
    "\n",
    "all_data[\"category_group\"] = pd.Series(pd.NA, index=all_data.index, dtype=\"object\")\n",
    "priority = [\"Dresses\",\"Swimwear\",\"Outerwear\",\"Footwear\",\"Activewear\",\"Accessories\"]  # specific -> broad\n",
    "\n",
    "for grp in priority:\n",
    "    m = all_data[\"category_group\"].isna() & all_data[\"_cat_norm\"].str.len().gt(0) & all_data[\"_cat_norm\"].apply(lambda t: bool(CAT_RX[grp].search(t)))\n",
    "    all_data.loc[m, \"category_group\"] = grp\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5) Stage-2: fallback on combined text (name/category/type)\n",
    "# -----------------------------------------------------------\n",
    "FB_RX = CAT_RX  # reuse same vocab\n",
    "def fallback_label(t: str):\n",
    "    for grp in priority:\n",
    "        if FB_RX[grp].search(t):\n",
    "            return grp\n",
    "    return pd.NA\n",
    "\n",
    "mask_unlabeled = all_data[\"category_group\"].isna()\n",
    "all_data.loc[mask_unlabeled, \"category_group\"] = all_data.loc[mask_unlabeled, \"_text\"].apply(fallback_label)\n",
    "\n",
    "# final fill\n",
    "all_data[\"category_group\"] = all_data[\"category_group\"].fillna(\"Other\")\n",
    "\n",
    "# --------------------------------\n",
    "# 6) Save & basic sanity reporting\n",
    "# --------------------------------\n",
    "all_data.drop(columns=[\"_text\",\"_cat_norm\"], inplace=True, errors=\"ignore\")\n",
    "all_data.to_csv(OUT_MASTER, index=False)\n",
    "\n",
    "print(\"Saved master ->\", OUT_MASTER)\n",
    "print(\"\\nCategory counts:\\n\", all_data[\"category_group\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "print(\"\\nMissing product_name by source:\")\n",
    "print(all_data.groupby(\"source_file\")[\"product_name\"].apply(lambda s: s.isna().sum()).to_string())\n",
    "\n",
    "# Export one CSV per bucket (optional but handy)\n",
    "for grp, g in all_data.groupby(all_data[\"category_group\"].fillna(\"Other\")):\n",
    "    g.to_csv(OUT_DIR_BUCKETS / f\"{grp}.csv\", index=False)\n",
    "\n",
    "# Peek at any remaining 'Other'\n",
    "others_preview = all_data.loc[all_data[\"category_group\"].eq(\"Other\"), [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"]].head(20)\n",
    "print(\"\\nSample of remaining 'Other':\")\n",
    "print(others_preview.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb08fd94-1f76-4b78-8f32-2d2991b134d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir: /Users/VATSAL/Clozyt recommender system\n",
      "Files in dir:\n",
      " - all_products_cleaned_master.csv\n",
      " - all_products_cleaned_master_6groups.csv\n",
      " - all_products_cleaned_master_with_groups.csv\n",
      " - alo_yoga_products.csv\n",
      " - alo_yoga_products_cleaned.csv\n",
      " - altardstate_products.csv\n",
      " - altardstate_products_cleaned.csv\n",
      " - cupshe_products.csv\n",
      " - cupshe_products_cleaned.csv\n",
      " - edikted_products.csv\n",
      " - edikted_products_cleaned.csv\n",
      " - gymshark_products.csv\n",
      " - gymshark_products_cleaned.csv\n",
      " - nakd_products.csv\n",
      " - nakd_products_cleaned.csv\n",
      " - princess_polly.csv\n",
      " - princess_polly_cleaned.csv\n",
      " - vuori_products.csv\n",
      " - vuori_products_cleaned.csv\n",
      "\n",
      "Missing from expected list: []\n",
      "OK  alo_yoga_products_cleaned.csv       -> shape sample (3, 6)\n",
      "OK  altardstate_products_cleaned.csv    -> shape sample (3, 13)\n",
      "OK  cupshe_products_cleaned.csv         -> shape sample (3, 6)\n",
      "OK  edikted_products_cleaned.csv        -> shape sample (3, 10)\n",
      "OK  gymshark_products_cleaned.csv       -> shape sample (3, 9)\n",
      "OK  nakd_products_cleaned.csv           -> shape sample (3, 7)\n",
      "OK  princess_polly_cleaned.csv          -> shape sample (3, 7)\n",
      "OK  vuori_products_cleaned.csv          -> shape sample (3, 7)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"./\")  # adjust if your CSVs are elsewhere\n",
    "\n",
    "print(\"Dir:\", DATA_DIR.resolve())\n",
    "print(\"Files in dir:\")\n",
    "for p in sorted(DATA_DIR.glob(\"*.csv\")):\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "expected = [\n",
    "    \"alo_yoga_products_cleaned.csv\",\n",
    "    \"altardstate_products_cleaned.csv\",\n",
    "    \"cupshe_products_cleaned.csv\",\n",
    "    \"edikted_products_cleaned.csv\",\n",
    "    \"gymshark_products_cleaned.csv\",\n",
    "    \"nakd_products_cleaned.csv\",\n",
    "    \"princess_polly_cleaned.csv\",\n",
    "    \"vuori_products_cleaned.csv\",\n",
    "]\n",
    "\n",
    "missing = [f for f in expected if not (DATA_DIR / f).exists()]\n",
    "print(\"\\nMissing from expected list:\", missing)\n",
    "\n",
    "# try reading each and report shape\n",
    "for f in expected:\n",
    "    p = DATA_DIR / f\n",
    "    if p.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(p, nrows=3)\n",
    "            print(f\"OK  {f:35s} -> shape sample {df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERR {f:35s} -> {type(e).__name__}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d021d3dd-8d93-4162-8c3e-974dfe8a2985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files: ['alo_yoga_products_cleaned.csv', 'altardstate_products_cleaned.csv', 'cupshe_products_cleaned.csv', 'edikted_products_cleaned.csv', 'gymshark_products_cleaned.csv', 'nakd_products_cleaned.csv', 'princess_polly_cleaned.csv', 'vuori_products_cleaned.csv']\n",
      "Saved master -> all_products_cleaned_master_6groups.csv\n",
      "\n",
      "Category counts:\n",
      " category_group\n",
      "Activewear     3033\n",
      "Dresses        2415\n",
      "Other          1249\n",
      "Outerwear       331\n",
      "Accessories     208\n",
      "Swimwear         80\n",
      "Footwear         59\n",
      "\n",
      "Missing product_name by source:\n",
      "source_file\n",
      "alo_yoga_products_cleaned.csv       0\n",
      "altardstate_products_cleaned.csv    0\n",
      "cupshe_products_cleaned.csv         0\n",
      "edikted_products_cleaned.csv        0\n",
      "gymshark_products_cleaned.csv       0\n",
      "nakd_products_cleaned.csv           0\n",
      "princess_polly_cleaned.csv          0\n",
      "vuori_products_cleaned.csv          0\n",
      "\n",
      "Sample of any remaining 'Other':\n",
      "                    product_name category product_type         brand                      source_file\n",
      "           Cashmere Jet Set Crew      NaN         <NA>      Alo Yoga    alo_yoga_products_cleaned.csv\n",
      "          Allyson Striped Zip Up Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "                 Sadie Denim Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "           Harper Boho Cinch Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "   Caroline Ruffle Tie Front Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "      Loren Ribbed Mock-Neck Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "      Loren Ribbed Mock-Neck Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "             Irene Fringe Duster Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "   Katherine High Rise Seam Jean Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "             Irene Fringe Duster Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "             Irene Fringe Duster Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "      Kelcie Floral Babydoll Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "       Harley Ribbed Layered Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "           Wilma Mixed Button-Up Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "Georgia Ruched Seamless Tube Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "            Brixton Boxy Thermal Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "       Harley Ribbed Layered Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "Georgia Ruched Seamless Tube Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "            Brixton Boxy Thermal Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "              Corwin Gingham Top Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# === MERGE (glob) + SAFE product_name + 6-GROUP CLASSIFICATION ===\n",
    "import pandas as pd, numpy as np, re, unicodedata\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "DATA_DIR = Path(\"./\")\n",
    "OUT_MASTER = \"all_products_cleaned_master_6groups.csv\"\n",
    "OUT_DIR = Path(\"by_category_groups_6\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Map filename stem -> brand (fallback if 'brand' column missing/empty)\n",
    "BRAND_MAP = {\n",
    "    \"alo_yoga_products_cleaned\": \"Alo Yoga\",\n",
    "    \"altardstate_products_cleaned\": \"ALTAR'D STATE\",\n",
    "    \"cupshe_products_cleaned\": \"Cupshe\",\n",
    "    \"edikted_products_cleaned\": \"Edikted\",\n",
    "    \"gymshark_products_cleaned\": \"Gymshark\",\n",
    "    \"nakd_products_cleaned\": \"NA-KD\",\n",
    "    \"princess_polly_cleaned\": \"Princess Polly\",\n",
    "    \"vuori_products_cleaned\": \"Vuori\",\n",
    "}\n",
    "\n",
    "def canon_colname(c: str) -> str:\n",
    "    raw = unicodedata.normalize(\"NFKC\", str(c)).strip().lower()\n",
    "    raw = raw.replace(\"/\", \" \").replace(\"\\\\\", \" \")\n",
    "    raw = re.sub(r\"\\s+\", \" \", raw)\n",
    "    tmp = raw.replace(\"(\", \"_\").replace(\")\", \"_\")\n",
    "    tmp = re.sub(r\"[^a-z0-9]+\", \"_\", tmp).strip(\"_\")\n",
    "    aliases = {\n",
    "        \"productname\":\"product_name\",\"product_name\":\"product_name\",\"name\":\"product_name\",\"title\":\"product_name\",\n",
    "        \"price\":\"price\",\"price_usd\":\"price\",\"priceusd\":\"price\",\"price_us\":\"price\",\n",
    "        \"image_url\":\"image_url\",\"product_url\":\"url\",\"url\":\"url\",\"link\":\"url\",\n",
    "        \"category\":\"category\",\"product_category\":\"category\",\"categories\":\"category\",\"taxonomy\":\"category\",\n",
    "        \"product_type\":\"product_type\",\"type\":\"product_type\",\n",
    "        \"sku\":\"sku\",\"id\":\"sku\",\"product_id\":\"sku\",\n",
    "    }\n",
    "    if \"price_usd\" in tmp or raw.strip()==\"price(usd)\":\n",
    "        return \"price\"\n",
    "    return aliases.get(tmp, tmp)\n",
    "\n",
    "frames, loaded = [], []\n",
    "for p in sorted(DATA_DIR.glob(\"*_cleaned.csv\")):\n",
    "    df = pd.read_csv(p)\n",
    "    df.columns = [canon_colname(c) for c in df.columns]\n",
    "\n",
    "    # coalesce duplicate-named columns (keep first non-empty)\n",
    "    if df.columns.duplicated().any():\n",
    "        groups = defaultdict(list)\n",
    "        for i, c in enumerate(df.columns):\n",
    "            groups[c].append(i)\n",
    "        new_df = pd.DataFrame(index=df.index)\n",
    "        for c, idxs in groups.items():\n",
    "            if len(idxs)==1:\n",
    "                new_df[c] = df.iloc[:, idxs[0]]\n",
    "            else:\n",
    "                s = df.iloc[:, idxs[0]]\n",
    "                for j in idxs[1:]:\n",
    "                    cand = df.iloc[:, j]\n",
    "                    empty = s.isna() | (s.astype(str).str.strip().isin([\"\", \"nan\", \"None\"]))\n",
    "                    s = s.where(~empty, cand)\n",
    "                new_df[c] = s\n",
    "        df = new_df\n",
    "\n",
    "    df[\"source_file\"] = p.name\n",
    "    if \"brand\" not in df.columns or df[\"brand\"].isna().all():\n",
    "        df[\"brand\"] = BRAND_MAP.get(p.stem, p.stem)\n",
    "\n",
    "    frames.append(df)\n",
    "    loaded.append(p.name)\n",
    "\n",
    "assert frames, \"No *_cleaned.csv files were loaded.\"\n",
    "all_data = pd.concat(frames, ignore_index=True, sort=False)\n",
    "\n",
    "# Ensure required columns exist in all_data\n",
    "for c in [\"product_name\", \"category\", \"product_type\", \"brand\", \"source_file\"]:\n",
    "    if c not in all_data.columns:\n",
    "        all_data[c] = pd.NA\n",
    "\n",
    "# SAFE coalesce for product_name (do not wipe existing values)\n",
    "if \"product_name\" not in all_data.columns:\n",
    "    all_data[\"product_name\"] = pd.NA\n",
    "for alt in [\"name\",\"title\"]:\n",
    "    if alt in all_data.columns:\n",
    "        mask = all_data[\"product_name\"].isna() | (all_data[\"product_name\"].astype(str).str.strip()==\"\")\n",
    "        all_data.loc[mask, \"product_name\"] = all_data.loc[mask, alt]\n",
    "if \"url\" in all_data.columns:\n",
    "    mask = all_data[\"product_name\"].isna()\n",
    "    all_data.loc[mask, \"product_name\"] = (\n",
    "        all_data.loc[mask, \"url\"].astype(str)\n",
    "        .str.extract(r\"/([^/?#]+)(?:\\?.*)?$\")[0]\n",
    "        .str.replace(\"-\", \" \", regex=False).str.replace(\"_\", \" \", regex=False)\n",
    "    )\n",
    "all_data[\"product_name\"] = (\n",
    "    all_data[\"product_name\"].astype(str).str.strip()\n",
    "    .replace({\"\": pd.NA, \"nan\": pd.NA, \"None\": pd.NA})\n",
    ")\n",
    "\n",
    "# --- normalized text for classification ---\n",
    "def norm_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "all_data[\"_cat_norm\"] = all_data.get(\"category\", pd.Series(\"\", index=all_data.index)).apply(norm_text)\n",
    "all_data[\"_text\"] = (\n",
    "    all_data.get(\"product_name\", pd.Series(\"\", index=all_data.index)).apply(norm_text)\n",
    "    + \" \" + all_data.get(\"category\", pd.Series(\"\", index=all_data.index)).apply(norm_text)\n",
    "    + \" \" + all_data.get(\"product_type\", pd.Series(\"\", index=all_data.index)).apply(norm_text)\n",
    ").str.strip()\n",
    "\n",
    "# --- 6-bucket vocab & regex (category-first, then fallback) ---\n",
    "def plural_rx(word: str) -> str:\n",
    "    if word.endswith((\"ss\",\"ch\",\"sh\")): return rf\"{re.escape(word)}(?:es)?\"\n",
    "    elif word.endswith(\"y\"):           return rf\"{re.escape(word)}s?\"\n",
    "    else:                              return rf\"{re.escape(word)}s?\"\n",
    "\n",
    "def build_or_rx(words):\n",
    "    toks=[]\n",
    "    for w in words:\n",
    "        w = w.replace(\"-\", \" \")\n",
    "        toks.append(plural_rx(w) if \" \" not in w else re.escape(w))\n",
    "    return re.compile(rf\"(?:^|\\s)(?:{'|'.join(toks)})(?:\\s|$)\", re.I)\n",
    "\n",
    "CAT_RULES = {\n",
    "    \"Dresses\":   [\"dress\"],\n",
    "    \"Swimwear\":  [\"bikini\",\"one piece\",\"swimsuit\",\"coverup\",\"cover up\"],\n",
    "    \"Outerwear\": [\"jacket\",\"coat\",\"bomber\",\"blazer\",\"trench\",\"parka\",\"puffer\",\"windbreaker\",\"shacket\",\"gilet\",\"vest\"],\n",
    "    \"Footwear\":  [\"boot\",\"sneaker\",\"runner\",\"sandal\",\"slide\",\"loafer\",\"flat\",\"ballerina\",\"slipper\"],\n",
    "    \"Activewear\":[\"legging\",\"jogger\",\"sweatpant\",\"pant\",\"trouser\",\"capri\",\"short\",\"skirt\",\"skort\",\n",
    "                  \"tee\",\"t shirt\",\"tank\",\"long sleeve\",\"henley\",\"polo\",\"button down\",\"crewneck\",\n",
    "                  \"bra\",\"sports bra\",\"hoodie\",\"sweater\",\"sweatshirt\",\"pullover\",\"cardigan\",\n",
    "                  \"onesie\",\"jumpsuit\",\"romper\",\"unitard\",\"bodysuit\",\"thong\",\"brief\",\"boxer\",\"boyshort\"],\n",
    "    \"Accessories\":[\"beanie\",\"hat\",\"cap\",\"visor\",\"headband\",\"scrunchie\",\"glove\",\"mitten\",\"scarf\",\"belt\",\n",
    "                   \"clip\",\"claw clip\",\"wristband\",\"hair tie\",\"mat\",\"yoga mat\",\"bag\",\"tote\",\"backpack\",\n",
    "                   \"duffel\",\"duffle\",\"water bottle\",\"bottle\",\"towel\",\"sock\"],\n",
    "}\n",
    "RX = {k: build_or_rx(v) for k,v in CAT_RULES.items()}\n",
    "PRIORITY = [\"Dresses\",\"Swimwear\",\"Outerwear\",\"Footwear\",\"Activewear\",\"Accessories\"]\n",
    "\n",
    "all_data[\"category_group\"] = pd.Series(pd.NA, index=all_data.index, dtype=\"object\")\n",
    "for grp in PRIORITY:\n",
    "    m = all_data[\"category_group\"].isna() & all_data[\"_cat_norm\"].str.len().gt(0) & all_data[\"_cat_norm\"].apply(lambda t: bool(RX[grp].search(t)))\n",
    "    all_data.loc[m, \"category_group\"] = grp\n",
    "\n",
    "mask = all_data[\"category_group\"].isna()\n",
    "def fb(t: str):\n",
    "    for grp in PRIORITY:\n",
    "        if RX[grp].search(t): return grp\n",
    "    return pd.NA\n",
    "all_data.loc[mask, \"category_group\"] = all_data.loc[mask, \"_text\"].apply(fb)\n",
    "all_data[\"category_group\"] = all_data[\"category_group\"].fillna(\"Other\")\n",
    "\n",
    "# --- save & report ---\n",
    "all_data.drop(columns=[\"_text\",\"_cat_norm\"], errors=\"ignore\", inplace=True)\n",
    "all_data.to_csv(OUT_MASTER, index=False)\n",
    "for grp, g in all_data.groupby(all_data[\"category_group\"].fillna(\"Other\")):\n",
    "    g.to_csv(OUT_DIR / f\"{grp}.csv\", index=False)\n",
    "\n",
    "print(\"Loaded files:\", loaded)\n",
    "print(\"Saved master ->\", OUT_MASTER)\n",
    "print(\"\\nCategory counts:\\n\", all_data[\"category_group\"].value_counts(dropna=False).to_string())\n",
    "print(\"\\nMissing product_name by source:\")\n",
    "print(all_data.groupby(\"source_file\")[\"product_name\"].apply(lambda s: s.isna().sum()).to_string())\n",
    "\n",
    "# Safe preview: only select columns that actually exist\n",
    "safe_cols = [c for c in [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"] if c in all_data.columns]\n",
    "print(\"\\nSample of any remaining 'Other':\")\n",
    "print(all_data.loc[all_data[\"category_group\"].eq(\"Other\"), safe_cols].head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76db290f-38b0-4eb6-9734-625457933858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Activewear     3773\n",
      "Dresses        2415\n",
      "Other           502\n",
      "Outerwear       338\n",
      "Accessories     208\n",
      "Swimwear         80\n",
      "Footwear         59\n",
      "\n",
      "Remaining 'Other' sample:\n",
      "                  product_name category product_type         brand                      source_file\n",
      "         Cashmere Jet Set Crew      NaN         <NA>      Alo Yoga    alo_yoga_products_cleaned.csv\n",
      "        Allyson Striped Zip Up Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      " Katherine High Rise Seam Jean Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "     Kaylie Straight Leg Jeans Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "       Karson Distressed Jeans Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "      Tracie Waffle Knit Tunic Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "           Paizley Lace Blouse Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "      Tracie Waffle Knit Tunic Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "Grae Mid-Rise Distressed Jeans Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "      Tracie Waffle Knit Tunic Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "  Armani Flutter Sleeve Blouse Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "            Birdie Lace Blouse Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "        Rena Aztec Quarter Zip Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      " Anslie Star Embroidered Jeans Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "          Lyra Crop Flare Jean Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "            Callan Floral Cami Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      " Rose High Rise Corduroy Flare Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "     Sophia Relaxed Baggy Jean Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "  Fleur Longline Lace Bralette Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "   Odette Lace Halter Bralette Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Re-categorize current 'Other' with extra terms ---\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def norm_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "# build a searchable text for only 'Other' rows\n",
    "mask_other = all_data[\"category_group\"].eq(\"Other\")\n",
    "other_text = (\n",
    "    all_data.loc[mask_other, \"product_name\"].apply(norm_text).fillna(\"\") + \" \" +\n",
    "    all_data.loc[mask_other, \"category\"].apply(norm_text).fillna(\"\") + \" \" +\n",
    "    all_data.loc[mask_other, \"product_type\"].apply(norm_text).fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "# NEW tokens:\n",
    "# - Activewear: top(s), button up, mock neck, thermal, seamless\n",
    "# - Outerwear: duster, kimono, coatigan\n",
    "EXTRA = {\n",
    "    \"Activewear\": re.compile(\n",
    "        r\"(?:^|\\s)(top|tops|button up|mock neck|thermal|seamless)(?:\\s|$)\", re.I\n",
    "    ),\n",
    "    \"Outerwear\": re.compile(\n",
    "        r\"(?:^|\\s)(duster|kimono|coatigan)(?:\\s|$)\", re.I\n",
    "    ),\n",
    "}\n",
    "\n",
    "# priority: map Outerwear before Activewear (so \"duster top\" becomes Outerwear)\n",
    "def recat(txt: str) -> str:\n",
    "    if EXTRA[\"Outerwear\"].search(txt):   return \"Outerwear\"\n",
    "    if EXTRA[\"Activewear\"].search(txt):  return \"Activewear\"\n",
    "    return \"Other\"\n",
    "\n",
    "# apply just to 'Other'\n",
    "new_labels = other_text.apply(recat)\n",
    "all_data.loc[mask_other & new_labels.ne(\"Other\"), \"category_group\"] = new_labels[new_labels.ne(\"Other\")]\n",
    "\n",
    "# report\n",
    "print(all_data[\"category_group\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "# peek at what remains in Other\n",
    "safe_cols = [c for c in [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"] if c in all_data.columns]\n",
    "print(\"\\nRemaining 'Other' sample:\")\n",
    "print(all_data.loc[all_data[\"category_group\"].eq(\"Other\"), safe_cols].head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c81867b9-50e6-41dd-b971-94ab0e57f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Dresses        2376\n",
      "Activewear     2217\n",
      "Casualwear     1853\n",
      "Outerwear       313\n",
      "Other           251\n",
      "Accessories     202\n",
      "Innerwear        56\n",
      "Footwear         55\n",
      "Swimwear         52\n",
      "\n",
      "Remaining 'Other' sample:\n",
      "                   product_name category product_type         brand                      source_file\n",
      "          Cashmere Jet Set Crew      NaN         <NA>      Alo Yoga    alo_yoga_products_cleaned.csv\n",
      "         Allyson Striped Zip Up Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "         Rena Aztec Quarter Zip Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "         Mollie Pointelle Shrug Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "       Waverly Stripe Crew Neck Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "        Layla Ribbon Trim Shrug Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "   New York Varsity Quarter Zip Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "Maria Pointelle Lace Trim Shrug Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "Maria Pointelle Lace Trim Shrug Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "       Wildflower Sherpa Zip Up Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "       Wildflower Sherpa Zip Up Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "       Supersoft Camo Crew Neck Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "                Easygoing Layer Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "                Easygoing Layer Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "               Step Aside Layer Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "                Roll Call Layer Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "                Roll Call Layer Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "                Roll Call Layer Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "             Swift Stripe Layer Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "             Swift Stripe Layer Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "\n",
      "Saved master -> all_products_cleaned_master_8groups.csv\n",
      "Per-bucket CSVs -> by_category_groups_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:37: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask_inner = txt_all.str.contains(RX_INNERWEAR)\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:41: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask_zip_outer = (~mask_inner) & txt_all.str.contains(RX_ZIP) & txt_all.str.contains(RX_OUTERWEAR_NOUN)\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask_zip_top = (~mask_inner) & (~mask_zip_outer) & txt_all.str.contains(RX_ZIP) & txt_all.str.contains(RX_ZIP_TOP)\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:49: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask_casual = (~mask_inner) & (~mask_zip_outer) & (~mask_zip_top) & (txt_all.str.contains(RX_CASUAL_DENIM) | txt_all.str.contains(RX_CASUAL_TOPS))\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:55: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  all_data.loc[mask_other & txt_other.str.contains(RX_INNERWEAR), \"category_group\"] = \"Innerwear\"\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:56: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  all_data.loc[mask_other & txt_other.str.contains(RX_ZIP) & txt_other.str.contains(RX_OUTERWEAR_NOUN), \"category_group\"] = \"Outerwear\"\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:56: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  all_data.loc[mask_other & txt_other.str.contains(RX_ZIP) & txt_other.str.contains(RX_OUTERWEAR_NOUN), \"category_group\"] = \"Outerwear\"\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:57: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  all_data.loc[mask_other & (~txt_other.str.contains(RX_ZIP) | ~txt_other.str.contains(RX_OUTERWEAR_NOUN)) & txt_other.str.contains(RX_ZIP) & txt_other.str.contains(RX_ZIP_TOP), \"category_group\"] = \"Activewear\"\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:57: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  all_data.loc[mask_other & (~txt_other.str.contains(RX_ZIP) | ~txt_other.str.contains(RX_OUTERWEAR_NOUN)) & txt_other.str.contains(RX_ZIP) & txt_other.str.contains(RX_ZIP_TOP), \"category_group\"] = \"Activewear\"\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:57: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  all_data.loc[mask_other & (~txt_other.str.contains(RX_ZIP) | ~txt_other.str.contains(RX_OUTERWEAR_NOUN)) & txt_other.str.contains(RX_ZIP) & txt_other.str.contains(RX_ZIP_TOP), \"category_group\"] = \"Activewear\"\n",
      "/var/folders/gn/zn5qy7wj2cdgmrqq3n9tgk840000gn/T/ipykernel_31831/775697339.py:58: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  all_data.loc[mask_other & (txt_other.str.contains(RX_CASUAL_DENIM) | txt_other.str.contains(RX_CASUAL_TOPS)), \"category_group\"] = \"Casualwear\"\n"
     ]
    }
   ],
   "source": [
    "# --- Add CASUALWEAR bucket and re-route items context-aware ---\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def norm(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "txt_all = (\n",
    "    all_data.get(\"product_name\", pd.Series(\"\", index=all_data.index)).apply(norm) + \" \" +\n",
    "    all_data.get(\"category\", pd.Series(\"\", index=all_data.index)).apply(norm) + \" \" +\n",
    "    all_data.get(\"product_type\", pd.Series(\"\", index=all_data.index)).apply(norm)\n",
    ").str.strip()\n",
    "\n",
    "# Buckets:\n",
    "# Innerwear (keep if you already added it earlier; otherwise skip lines with \"Innerwear\")\n",
    "RX_INNERWEAR = re.compile(r\"(?:^|\\s)(bralette|lingerie|underwear|briefs?|thongs?|pant(y|ies)|boyshorts?|bodysuits?|unitard|catsuit)(?:\\s|$)\", re.I)\n",
    "\n",
    "# Zip/outerwear nouns => Outerwear\n",
    "RX_ZIP = re.compile(r\"(?:^|\\s)(zip\\s*up|zipup|zip|quarter\\s*zip|half\\s*zip|1\\/4\\s*zip)(?:\\s|$)\", re.I)\n",
    "RX_OUTERWEAR_NOUN = re.compile(r\"(?:^|\\s)(jacket|coat|parka|puffer|shell|windbreaker|anorak|tren(ch|chcoat)|gilet|vest|bomber|blazer)(?:\\s|$)\", re.I)\n",
    "\n",
    "# Zip + athletic tops => Activewear\n",
    "RX_ZIP_TOP = re.compile(r\"(?:^|\\s)(hoodie|fleece|sweater|sweatshirt|pullover|top)(?:\\s|$)\", re.I)\n",
    "\n",
    "# CASUALWEAR: denim bottoms + everyday tops/blouses/tunics/camis/shirts\n",
    "RX_CASUAL_DENIM = re.compile(r\"(?:^|\\s)(jeans?|denim|flare|bootcut|straight\\s*leg|baggy)(?:\\s|$)\", re.I)\n",
    "RX_CASUAL_TOPS  = re.compile(r\"(?:^|\\s)(blouse|tunic|cami|camisole|shirt|button\\s*up|mock\\s*neck|thermal|seamless|top|tops)(?:\\s|$)\", re.I)\n",
    "\n",
    "# --- precedence rules ---\n",
    "# 1) Innerwear wins\n",
    "mask_inner = txt_all.str.contains(RX_INNERWEAR)\n",
    "all_data.loc[mask_inner, \"category_group\"] = \"Innerwear\"\n",
    "\n",
    "# 2) Zip + outerwear noun -> Outerwear\n",
    "mask_zip_outer = (~mask_inner) & txt_all.str.contains(RX_ZIP) & txt_all.str.contains(RX_OUTERWEAR_NOUN)\n",
    "all_data.loc[mask_zip_outer, \"category_group\"] = \"Outerwear\"\n",
    "\n",
    "# 3) Zip + athletic top -> Activewear\n",
    "mask_zip_top = (~mask_inner) & (~mask_zip_outer) & txt_all.str.contains(RX_ZIP) & txt_all.str.contains(RX_ZIP_TOP)\n",
    "all_data.loc[mask_zip_top, \"category_group\"] = \"Activewear\"\n",
    "\n",
    "# 4) Denim bottoms or casual tops -> Casualwear (only if not already labeled by above)\n",
    "mask_casual = (~mask_inner) & (~mask_zip_outer) & (~mask_zip_top) & (txt_all.str.contains(RX_CASUAL_DENIM) | txt_all.str.contains(RX_CASUAL_TOPS))\n",
    "all_data.loc[mask_casual, \"category_group\"] = \"Casualwear\"\n",
    "\n",
    "# 5) Re-sweep any remaining 'Other' with same logic\n",
    "mask_other = all_data[\"category_group\"].eq(\"Other\")\n",
    "txt_other = txt_all[mask_other]\n",
    "all_data.loc[mask_other & txt_other.str.contains(RX_INNERWEAR), \"category_group\"] = \"Innerwear\"\n",
    "all_data.loc[mask_other & txt_other.str.contains(RX_ZIP) & txt_other.str.contains(RX_OUTERWEAR_NOUN), \"category_group\"] = \"Outerwear\"\n",
    "all_data.loc[mask_other & (~txt_other.str.contains(RX_ZIP) | ~txt_other.str.contains(RX_OUTERWEAR_NOUN)) & txt_other.str.contains(RX_ZIP) & txt_other.str.contains(RX_ZIP_TOP), \"category_group\"] = \"Activewear\"\n",
    "all_data.loc[mask_other & (txt_other.str.contains(RX_CASUAL_DENIM) | txt_other.str.contains(RX_CASUAL_TOPS)), \"category_group\"] = \"Casualwear\"\n",
    "\n",
    "# Report + save\n",
    "print(all_data[\"category_group\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "safe_cols = [c for c in [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"] if c in all_data.columns]\n",
    "print(\"\\nRemaining 'Other' sample:\")\n",
    "print(all_data.loc[all_data[\"category_group\"].eq(\"Other\"), safe_cols].head(20).to_string(index=False))\n",
    "\n",
    "# Persist (new master + per-bucket files)\n",
    "out_master = \"all_products_cleaned_master_8groups.csv\"  # 6 original + Accessories + Casualwear (+ Innerwear if you added earlier)\n",
    "all_data.to_csv(out_master, index=False)\n",
    "outdir = Path(\"by_category_groups_8\")\n",
    "outdir.mkdir(exist_ok=True)\n",
    "for grp, g in all_data.groupby(all_data[\"category_group\"].fillna(\"Other\")):\n",
    "    g.to_csv(outdir / f\"{grp}.csv\", index=False)\n",
    "print(\"\\nSaved master ->\", out_master)\n",
    "print(\"Per-bucket CSVs ->\", outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7a8ec81-ea70-4d68-8c0b-b56d2e4a1e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_group\n",
      "Dresses        2376\n",
      "Activewear     2228\n",
      "Casualwear     1861\n",
      "Outerwear       322\n",
      "Other           223\n",
      "Accessories     202\n",
      "Innerwear        56\n",
      "Footwear         55\n",
      "Swimwear         52\n",
      "\n",
      "Remaining 'Other' sample:\n",
      "                                             product_name category product_type         brand                      source_file\n",
      "                                   Allyson Striped Zip Up Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "                                   Rena Aztec Quarter Zip Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "                             New York Varsity Quarter Zip Clothing         <NA> ALTAR'D STATE altardstate_products_cleaned.csv\n",
      "                                    Petal Motion Full Zip Clothing         <NA>    AS REVIVAL altardstate_products_cleaned.csv\n",
      "Classic Leopard Print Cross Back Midkini & High Waist Set      NaN         <NA>        Cupshe      cupshe_products_cleaned.csv\n",
      "                             Sizzling Hot Red Tankini Set      NaN         <NA>        Cupshe      cupshe_products_cleaned.csv\n",
      "                              Lou Crossover Beaded Corset      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                     Indira Printed Cupped Lace Up Corset      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                          Marcella Wide Strap Mesh Corset      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                                    Selena Lace Up Corset      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                                Kamila Fringe Knit Poncho      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                                Sequin Fringe Knit Poncho      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                         Maeve Pinstripe Strapless Corset      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                          Sheer Chiffon Asymmetric Poncho      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                         Ziva Faux Leather Lace Up Corset      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                            Azalia Asymmetric Knit Poncho      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                             Fringe Sequin Crochet Poncho      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                          Marcella Wide Strap Mesh Corset      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                                    Selena Lace Up Corset      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "                          Marcella Wide Strap Mesh Corset      NaN         <NA>       Edikted     edikted_products_cleaned.csv\n",
      "\n",
      "Updated master saved -> all_products_cleaned_master_8groups.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Final sweep of 'Other': shrug/sherpa/crew/layer ---\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def norm(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "mask_other = all_data[\"category_group\"].eq(\"Other\")\n",
    "txt = (\n",
    "    all_data.loc[mask_other, \"product_name\"].apply(norm).fillna(\"\") + \" \" +\n",
    "    all_data.loc[mask_other, \"category\"].apply(norm).fillna(\"\") + \" \" +\n",
    "    all_data.loc[mask_other, \"product_type\"].apply(norm).fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "# New rules:\n",
    "RX_OUTERWEAR_SHRUG  = re.compile(r\"(?:^|\\s)(shrug|sherpa)(?:\\s|$)\", re.I)        # treat as Outerwear\n",
    "RX_ACTIVE_CREW      = re.compile(r\"(?:^|\\s)(crew\\s*neck|crew)(?:\\s|$)\", re.I)    # sweaters/crews -> Activewear\n",
    "RX_CASUAL_LAYER     = re.compile(r\"(?:^|\\s)layer(ing)?(?:\\s|$)\", re.I)           # \"Layer Clothing\" -> Casualwear\n",
    "\n",
    "to_outer = txt.apply(lambda t: bool(RX_OUTERWEAR_SHRUG.search(t)))\n",
    "to_active = (~to_outer) & txt.apply(lambda t: bool(RX_ACTIVE_CREW.search(t)))\n",
    "to_casual = (~to_outer) & (~to_active) & txt.apply(lambda t: bool(RX_CASUAL_LAYER.search(t)))\n",
    "\n",
    "all_data.loc[mask_other & to_outer,  \"category_group\"] = \"Outerwear\"\n",
    "all_data.loc[mask_other & to_active, \"category_group\"] = \"Activewear\"\n",
    "all_data.loc[mask_other & to_casual, \"category_group\"] = \"Casualwear\"\n",
    "\n",
    "# report and save\n",
    "print(all_data[\"category_group\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "safe_cols = [c for c in [\"product_name\",\"category\",\"product_type\",\"brand\",\"source_file\"] if c in all_data.columns]\n",
    "print(\"\\nRemaining 'Other' sample:\")\n",
    "print(all_data.loc[all_data[\"category_group\"].eq(\"Other\"), safe_cols].head(20).to_string(index=False))\n",
    "\n",
    "all_data.to_csv(\"all_products_cleaned_master_8groups.csv\", index=False)\n",
    "print(\"\\nUpdated master saved -> all_products_cleaned_master_8groups.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "373ea3b2-87d5-4250-9846-c34c30261bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_other = all_data[\"category_group\"].eq(\"Other\")\n",
    "txt = all_data.loc[mask_other, \"product_name\"].fillna(\"\").str.lower()\n",
    "\n",
    "all_data.loc[mask_other & txt.str.contains(\"corset\"), \"category_group\"] = \"Innerwear\"\n",
    "all_data.loc[mask_other & txt.str.contains(\"poncho|shrug|cape\"), \"category_group\"] = \"Outerwear\"\n",
    "all_data.loc[mask_other & txt.str.contains(\"tankini|midkini|bikini\"), \"category_group\"] = \"Swimwear\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e4853c88-e8a4-45c0-a994-8b2108b276c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated category counts:\n",
      " category_group\n",
      "Dresses        2376\n",
      "Activewear     2228\n",
      "Casualwear     1861\n",
      "Outerwear       331\n",
      "Accessories     202\n",
      "Other           161\n",
      "Innerwear       107\n",
      "Footwear         55\n",
      "Swimwear         54\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Final clean-up sweep for 'Other'\n",
    "mask_other = all_data[\"category_group\"].eq(\"Other\")\n",
    "txt = all_data.loc[mask_other, \"product_name\"].fillna(\"\").str.lower()\n",
    "\n",
    "all_data.loc[mask_other & txt.str.contains(\"corset\"), \"category_group\"] = \"Innerwear\"\n",
    "all_data.loc[mask_other & txt.str.contains(\"poncho|shrug|cape\"), \"category_group\"] = \"Outerwear\"\n",
    "all_data.loc[mask_other & txt.str.contains(\"tankini|midkini|bikini\"), \"category_group\"] = \"Swimwear\"\n",
    "\n",
    "# Re-check distribution\n",
    "print(\"Updated category counts:\\n\", all_data[\"category_group\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d817244-5226-494b-8857-613e81c7ed76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid recommender ready. Try:\n",
      " - recommend('Airlift Intrigue Bra', topn=10, alpha=0.7)\n",
      " - recommend(0, topn=10, alpha=0.6)  # by row index\n"
     ]
    }
   ],
   "source": [
    "# HYBRID RECOMMENDER (content + pseudo-collab) for your master CSV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, unicodedata\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=4096)\n",
    "def _pseudo(i):  # cache collab scores\n",
    "    return pseudo_collab_scores(i)\n",
    "    \n",
    "MASTER_CSV = \"all_products_cleaned_master_8groups.csv\"  # adjust if needed\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "\n",
    "import re, unicodedata\n",
    "\n",
    "def slug(s):\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# new column used for deduplication\n",
    "df[\"dedupe_key\"] = df[\"product_name\"].map(slug) + \"|\" + df[\"brand\"].map(slug)\n",
    "\n",
    "\n",
    "# --- Minimal schema safety\n",
    "for c in [\"product_name\",\"brand\",\"category_group\",\"available_colors_cleaned\",\"price\"]:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "# clean text helpers\n",
    "def norm(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\",\" \", s)\n",
    "    return re.sub(r\"\\s+\",\" \", s).strip()\n",
    "\n",
    "# fill sensible defaults\n",
    "df[\"product_name\"] = df[\"product_name\"].fillna(\"\").astype(str)\n",
    "df[\"brand\"] = df[\"brand\"].fillna(\"UnknownBrand\").astype(str)\n",
    "df[\"category_group\"] = df[\"category_group\"].fillna(\"UnknownCategory\").astype(str)\n",
    "df[\"available_colors_cleaned\"] = df[\"available_colors_cleaned\"].fillna(\"\").astype(str)\n",
    "\n",
    "# parse price (strip currency/etc.)\n",
    "def parse_price(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x)\n",
    "    s = re.sub(r\"[^\\d\\.]\", \"\", s)  # keep digits and dot\n",
    "    try:\n",
    "        return float(s) if s else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df[\"price_num\"] = df[\"price\"].apply(parse_price)\n",
    "\n",
    "# ---------- CONTENT VECTOR ----------\n",
    "# text: product_name + brand + category (brand/category get extra weight via duplication)\n",
    "content_text = (\n",
    "    df[\"product_name\"].map(norm) + \" \" +\n",
    "    df[\"brand\"].map(lambda s: (norm(s)+\" \")*2) +    # duplicate -> more weight\n",
    "    df[\"category_group\"].map(lambda s: (norm(s)+\" \")*2)\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=40000)\n",
    "X_tfidf = tfidf.fit_transform(content_text)\n",
    "\n",
    "# (optional) one-hot brand/category and append to content\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "X_ohe = ohe.fit_transform(df[[\"brand\",\"category_group\"]])\n",
    "X_content = hstack([X_tfidf, X_ohe]).tocsr()\n",
    "\n",
    "# --- Brand centroids in content space (for \"similar brands\" lookups) ---\n",
    "# --- Brand centroids in content space (robust: uses 1-D ndarrays, no np.matrix) ---\n",
    "import numpy as np\n",
    "\n",
    "brands = df[\"brand\"].astype(str).values\n",
    "unique_brands = np.unique(brands)\n",
    "\n",
    "brand_centroid = {}\n",
    "for b in unique_brands:\n",
    "    idx = np.where(brands == b)[0]\n",
    "    if len(idx) == 0:\n",
    "        continue\n",
    "    # 1 x n sparse mean -> convert to 1-D ndarray\n",
    "    v = X_content[idx].mean(axis=0)          # shape (1, n), type np.matrix / sparse\n",
    "    v = np.asarray(v).ravel()                # -> 1-D ndarray\n",
    "    nrm = np.linalg.norm(v)\n",
    "    if nrm > 0:\n",
    "        v = v / nrm\n",
    "    brand_centroid[b] = v\n",
    "\n",
    "def similar_brands(base_brand, topk=6):\n",
    "    \"\"\"Return top-k brands most similar to base brand in content space (including base).\"\"\"\n",
    "    if base_brand not in brand_centroid:\n",
    "        return [base_brand]\n",
    "    base = brand_centroid[base_brand]\n",
    "    sims = []\n",
    "    for b, vec in brand_centroid.items():\n",
    "        # cosine for L2-normalized vectors is just dot product\n",
    "        sim = float(np.dot(base, vec))\n",
    "        sims.append((b, sim))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [b for b, _ in sims[:topk]]\n",
    "\n",
    "# ---------- PSEUDO-COLLAB SIM ----------\n",
    "# brand/category match, color overlap (Jaccard), price proximity\n",
    "\n",
    "# preprocess colors as sets\n",
    "def colors_set(s):\n",
    "    toks = re.split(r\"[,\\|/]+|\\s{2,}\", str(s))\n",
    "    toks = [norm(t) for t in toks if norm(t)]\n",
    "    return set(toks)\n",
    "\n",
    "color_sets = df[\"available_colors_cleaned\"].apply(colors_set)\n",
    "\n",
    "# scale price into [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "price_scaled = pd.Series(\n",
    "    scaler.fit_transform(df[[\"price_num\"]]).reshape(-1),\n",
    "    index=df.index\n",
    ").fillna(0.5)  # unknown mid\n",
    "\n",
    "# on-the-fly pseudo-collab similarity between a query item and all items\n",
    "def pseudo_collab_scores(i):\n",
    "    # weights (tune if you like)\n",
    "    w_brand = 0.30\n",
    "    w_cat   = 0.35\n",
    "    w_color = 0.20\n",
    "    w_price = 0.15\n",
    "\n",
    "    same_brand = (df[\"brand\"].values == df.iloc[i][\"brand\"])\n",
    "    same_cat   = (df[\"category_group\"].values == df.iloc[i][\"category_group\"])\n",
    "\n",
    "    # color Jaccard\n",
    "    A = color_sets.iloc[i]\n",
    "    def jacc(B):\n",
    "        if not A and not B: return 0.0\n",
    "        inter = len(A & B)\n",
    "        union = len(A | B) if (A or B) else 1\n",
    "        return inter / union\n",
    "    color_sim = np.array([jacc(s) for s in color_sets])\n",
    "\n",
    "    # price similarity (closer is better)\n",
    "    pi = price_scaled.iloc[i]\n",
    "    price_sim = 1.0 - np.abs(price_scaled.values - pi)\n",
    "\n",
    "    # combine\n",
    "    s = (w_brand*same_brand.astype(float) +\n",
    "         w_cat*same_cat.astype(float) +\n",
    "         w_color*color_sim +\n",
    "         w_price*price_sim)\n",
    "\n",
    "    # normalize 0..1\n",
    "    s = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "    return s\n",
    "\n",
    "# ---------- HYBRID SCORE ----------\n",
    "def hybrid_scores(i, k=50, alpha=0.7):\n",
    "    \"\"\"\n",
    "    i: index of the query item in df\n",
    "    alpha: weight for content (0..1). (1-alpha) used for pseudo-collab.\n",
    "    returns: indices and scores for top-k (excluding i)\n",
    "    \"\"\"\n",
    "    # content neighbors\n",
    "    dists, idxs = nn_content.kneighbors(X_content[i], n_neighbors=min(k+1, X_content.shape[0]))\n",
    "    idxs = idxs[0]\n",
    "    dists = dists[0]\n",
    "    content_scores = 1.0 - dists  # cosine -> similarity\n",
    "\n",
    "    # map content scores to full-length array\n",
    "    cont_full = np.zeros(len(df))\n",
    "    cont_full[idxs] = content_scores\n",
    "\n",
    "    # pseudo-collab over all items\n",
    "    collab_full = _pseudo(int(i))\n",
    "\n",
    "\n",
    "    # hybrid\n",
    "    hybrid = alpha*cont_full + (1.0-alpha)*collab_full\n",
    "    # remove self\n",
    "    hybrid[i] = -1\n",
    "    # top-k\n",
    "    top_idx = np.argpartition(-hybrid, range(min(k, len(df)-1)))[:k]\n",
    "    top_idx = top_idx[np.argsort(-hybrid[top_idx])]\n",
    "    return top_idx, hybrid[top_idx], cont_full[top_idx], collab_full[top_idx]\n",
    "\n",
    "def find_index_by_name(query):\n",
    "    # fuzzy-ish: normalize and look for first contains\n",
    "    q = norm(query)\n",
    "    hits = df.index[df[\"product_name\"].map(lambda s: q in norm(s))]\n",
    "    return int(hits[0]) if len(hits) else None\n",
    "\n",
    "def recommend(query, topn=10, alpha=0.7,\n",
    "              show_cols=(\"product_name\",\"brand\",\"category_group\",\"price\"),\n",
    "              same_brand=True, same_category_or_price=True,\n",
    "              random_brand=False,            # NEW: enable brand-diverse randomization\n",
    "              brand_topk=6,                  # how many \"similar\" brands to consider\n",
    "              temperature=0.7,               # softmax temperature (lower=pick higher-score more often)\n",
    "              max_per_brand=2,               # cap items per brand in the final list\n",
    "              seed=None):\n",
    "    \"\"\"\n",
    "    query: product index (int) OR product name (str)\n",
    "    random_brand=True -> sample from similar brands using softmax over hybrid scores\n",
    "    \"\"\"\n",
    "    i = query if isinstance(query, int) else find_index_by_name(query)\n",
    "    if i is None:\n",
    "        print(\"No product matched your query.\")\n",
    "        return\n",
    "\n",
    "    # get many candidates first\n",
    "    idxs, h, c, u = hybrid_scores(i, k=max(topn*10, 100), alpha=alpha)\n",
    "\n",
    "    base_cat   = df.iloc[i][\"category_group\"]\n",
    "    base_price = df.iloc[i][\"price_num\"]\n",
    "    base_brand = df.iloc[i][\"brand\"]\n",
    "    query_key  = df.iloc[i][\"dedupe_key\"]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # --- helper filters\n",
    "    def ok_basic(j):\n",
    "        # optional brand constraint\n",
    "        cond_brand = (df.iloc[j][\"brand\"] == base_brand) if same_brand else True\n",
    "        # keep if same category OR within a price band\n",
    "        cond_cat = (df.iloc[j][\"category_group\"] == base_cat)\n",
    "        cond_price = (\n",
    "            pd.isna(base_price) or\n",
    "            pd.isna(df.iloc[j][\"price_num\"]) or\n",
    "            abs(df.iloc[j][\"price_num\"] - base_price) <= max(20, 0.5 * base_price)\n",
    "        )\n",
    "        return cond_brand and (cond_cat or (same_category_or_price and cond_price))\n",
    "\n",
    "    # --- candidate pool (apply basic filters first)\n",
    "    pool = [j for j in idxs if ok_basic(j)]\n",
    "    if not pool:\n",
    "        pool = list(idxs)  # fall back to raw neighbors if too strict\n",
    "\n",
    "    # --- optionally restrict to \"similar brands\" (for cross-brand variety)\n",
    " # optionally restrict to \"similar brands\", but fall back if variety is too low\n",
    "    if random_brand:\n",
    "        sim_brands = set(similar_brands(base_brand, topk=brand_topk))\n",
    "        pool_sim = [j for j in pool if (df.iloc[j][\"brand\"] in sim_brands)]\n",
    "    \n",
    "        # need at least enough brands to fill topn under the cap\n",
    "        need_brands = max(1, int(np.ceil(topn / max_per_brand)))\n",
    "        uniq_brands = len({df.iloc[j][\"brand\"] for j in pool_sim})\n",
    "    \n",
    "        pool = pool_sim if (len(pool_sim) >= topn or uniq_brands >= need_brands) else pool\n",
    "\n",
    "\n",
    "    # --- dedupe by (name+brand) and brand quota, with softmax sampling\n",
    "    keep, seen_keys = [], {query_key}\n",
    "    brand_counts = {}\n",
    "\n",
    "    # scores aligned to pool\n",
    "    score_map = {j: h[list(idxs).index(j)] if j in idxs[:len(h)] else 0.0 for j in pool}\n",
    "    # build arrays for softmax sampling\n",
    "    cand = np.array(pool, dtype=int)\n",
    "    scrs = np.array([score_map[j] for j in cand], dtype=float)\n",
    "\n",
    "    def softmax(x, T=1.0):\n",
    "        x = (x - x.max()) / max(T, 1e-6)\n",
    "        e = np.exp(x)\n",
    "        p = e / (e.sum() + 1e-12)\n",
    "        return p\n",
    "\n",
    "    # iterate sampling without replacement\n",
    "    while len(keep) < topn and len(cand) > 0:\n",
    "        if random_brand:\n",
    "            p = softmax(scrs, T=temperature)  # random-but-biased by score\n",
    "            pick_idx = rng.choice(len(cand), p=p)\n",
    "        else:\n",
    "            pick_idx = int(np.argmax(scrs))   # deterministic top\n",
    "        j = int(cand[pick_idx])\n",
    "\n",
    "        k = df.iloc[j][\"dedupe_key\"]\n",
    "        b = df.iloc[j][\"brand\"]\n",
    "\n",
    "        if k not in seen_keys and brand_counts.get(b, 0) < max_per_brand:\n",
    "            keep.append(j)\n",
    "            seen_keys.add(k)\n",
    "            brand_counts[b] = brand_counts.get(b, 0) + 1\n",
    "\n",
    "        # remove that candidate and continue\n",
    "        cand = np.delete(cand, pick_idx)\n",
    "        scrs = np.delete(scrs, pick_idx)\n",
    "\n",
    "    # fall back if we didn’t fill topn\n",
    "    if len(keep) < topn:\n",
    "        for j in pool:\n",
    "            if len(keep) >= topn: break\n",
    "            k = df.iloc[j][\"dedupe_key\"]; b = df.iloc[j][\"brand\"]\n",
    "            if k in seen_keys or brand_counts.get(b,0) >= max_per_brand: continue\n",
    "            keep.append(j); seen_keys.add(k); brand_counts[b]=brand_counts.get(b,0)+1\n",
    "\n",
    "    out = df.loc[keep, list(show_cols)].copy()\n",
    "    # map scores again for display\n",
    "    hybrid_full = dict(zip(idxs, h))\n",
    "    cont_full   = dict(zip(idxs, c))\n",
    "    coll_full   = dict(zip(idxs, u))\n",
    "    out.insert(0, \"score\",      np.round([hybrid_full.get(j, 0.0) for j in keep], 4))\n",
    "    out.insert(1, \"content_sim\",np.round([cont_full.get(j, 0.0)   for j in keep], 4))\n",
    "    out.insert(2, \"collab_sim\", np.round([coll_full.get(j, 0.0)   for j in keep], 4))\n",
    "\n",
    "    print(\"Query →\", df.iloc[i][[\"product_name\",\"brand\",\"category_group\",\"price\"]].to_dict())\n",
    "    display(out.reset_index(drop=True))\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "print(\"Hybrid recommender ready. Try:\")\n",
    "print(\" - recommend('Airlift Intrigue Bra', topn=10, alpha=0.7)\")\n",
    "print(\" - recommend(0, topn=10, alpha=0.6)  # by row index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "94853962-8b13-4e47-80fd-1ddb698c4962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query → {'product_name': 'Airlift Intrigue Bra', 'brand': 'Alo Yoga', 'category_group': 'Activewear', 'price': 68.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>content_sim</th>\n",
       "      <th>collab_sim</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>category_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.8408</td>\n",
       "      <td>5\" Airlift Energy Short</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8470</td>\n",
       "      <td>0.8296</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>High-Waist Airlift Legging</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8244</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.8228</td>\n",
       "      <td>Accolade Hoodie</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8310</td>\n",
       "      <td>0.8281</td>\n",
       "      <td>0.8376</td>\n",
       "      <td>Airbrush Better Together Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8173</td>\n",
       "      <td>0.8268</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>Conquer Revitalize Pant</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    score  content_sim  collab_sim                  product_name     brand  \\\n",
       "0  0.8300       0.8254      0.8408       5\" Airlift Energy Short  Alo Yoga   \n",
       "1  0.8470       0.8296      0.8876    High-Waist Airlift Legging  Alo Yoga   \n",
       "2  0.8244       0.8251      0.8228               Accolade Hoodie  Alo Yoga   \n",
       "3  0.8310       0.8281      0.8376  Airbrush Better Together Bra  Alo Yoga   \n",
       "4  0.8173       0.8268      0.7950       Conquer Revitalize Pant  Alo Yoga   \n",
       "\n",
       "  category_group  price  \n",
       "0     Activewear   88.0  \n",
       "1     Activewear  128.0  \n",
       "2     Activewear  138.0  \n",
       "3     Activewear   88.0  \n",
       "4     Activewear  118.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query → {'product_name': 'Airlift Intrigue Bra', 'brand': 'Alo Yoga', 'category_group': 'Activewear', 'price': 68.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>content_sim</th>\n",
       "      <th>collab_sim</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>category_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8869</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>Airlift Empower Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8756</td>\n",
       "      <td>0.8943</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>Alosoft Relay Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8689</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>0.8069</td>\n",
       "      <td>Airlift Divine Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    score  content_sim  collab_sim         product_name     brand  \\\n",
       "0  0.8869       0.9266      0.7942  Airlift Empower Bra  Alo Yoga   \n",
       "1  0.8756       0.8943      0.8321    Alosoft Relay Bra  Alo Yoga   \n",
       "2  0.8689       0.8955      0.8069   Airlift Divine Bra  Alo Yoga   \n",
       "\n",
       "  category_group  price  \n",
       "0     Activewear   98.0  \n",
       "1     Activewear   78.0  \n",
       "2     Activewear   68.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# allow more from the same brand\n",
    "recommend(\"Airlift Intrigue Bra\", 10, 0.7,\n",
    "          same_brand=False, random_brand=True,\n",
    "          brand_topk=12, max_per_brand=5, seed=42)\n",
    "\n",
    "# or: don't restrict to \"similar brands\" at all (keeps diversity cap only)\n",
    "recommend(\"Airlift Intrigue Bra\", 10, 0.7,\n",
    "          same_brand=False, random_brand=False,  # deterministic\n",
    "          max_per_brand=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "39ae3d66-5f77-4005-96f4-49a4d7944046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query → {'product_name': 'Airlift Intrigue Bra', 'brand': 'Alo Yoga', 'category_group': 'Activewear', 'price': 68.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>content_sim</th>\n",
       "      <th>collab_sim</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>category_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8869</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>Airlift Empower Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8756</td>\n",
       "      <td>0.8943</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>Alosoft Relay Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    score  content_sim  collab_sim         product_name     brand  \\\n",
       "0  0.8869       0.9266      0.7942  Airlift Empower Bra  Alo Yoga   \n",
       "1  0.8756       0.8943      0.8321    Alosoft Relay Bra  Alo Yoga   \n",
       "\n",
       "  category_group  price  \n",
       "0     Activewear   98.0  \n",
       "1     Activewear   78.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recommend(\"Airlift Intrigue Bra\", 10, 0.7,\n",
    "          same_brand=False,\n",
    "          random_brand=False,      # no similar-brand restriction\n",
    "          same_category_or_price=False,\n",
    "          # allow more per brand:\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8ffa0e4f-b32e-4b22-ade8-553b4d5de077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query → {'product_name': 'Airlift Intrigue Bra', 'brand': 'Alo Yoga', 'category_group': 'Activewear', 'price': 68.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>content_sim</th>\n",
       "      <th>collab_sim</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>category_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8869</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>Airlift Empower Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8756</td>\n",
       "      <td>0.8943</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>Alosoft Relay Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    score  content_sim  collab_sim         product_name     brand  \\\n",
       "0  0.8869       0.9266      0.7942  Airlift Empower Bra  Alo Yoga   \n",
       "1  0.8756       0.8943      0.8321    Alosoft Relay Bra  Alo Yoga   \n",
       "\n",
       "  category_group  price  \n",
       "0     Activewear   98.0  \n",
       "1     Activewear   78.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recommend(\"Airlift Intrigue Bra\", topn=10, alpha=0.7, same_category_or_price=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1fb49771-fc48-4f62-b46b-7926eb98e496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ recommend_v2 is ready. Try:\n",
      " - recommend_v2('Airlift Intrigue Bra', 10, 0.7, same_brand=False, random_brand=True, brand_topk=20, max_per_brand=3, same_category_or_price=False, seed=42)\n",
      " - recommend_v2(0, 12, 0.6)\n"
     ]
    }
   ],
   "source": [
    "# --- BRAND-DIVERSE, FALLBACK-AWARE RECOMMENDER (drop-in) ---\n",
    "def recommend_v2(query, topn=10, alpha=0.7,\n",
    "                 show_cols=(\"product_name\",\"brand\",\"category_group\",\"price\"),\n",
    "                 same_brand=True,\n",
    "                 same_category_or_price=True,\n",
    "                 random_brand=False,       # stochastic brand mix\n",
    "                 brand_topk=12,            # similar brands to consider\n",
    "                 temperature=0.7,          # softmax temp (lower = pick higher-score more often)\n",
    "                 max_per_brand=None,       # cap per brand; None = no cap\n",
    "                 seed=None):\n",
    "    \"\"\"\n",
    "    Self-contained re-ranker that:\n",
    "      • pulls a *large* candidate pool,\n",
    "      • optionally restricts to similar brands (with fallback),\n",
    "      • relaxes filters if pool is too small,\n",
    "      • dedupes by name+brand,\n",
    "      • optionally samples to diversify brands.\n",
    "    Requires: df, find_index_by_name(), hybrid_scores(), similar_brands() and df['dedupe_key'] defined earlier.\n",
    "    \"\"\"\n",
    "\n",
    "    i = query if isinstance(query, int) else find_index_by_name(query)\n",
    "    if i is None:\n",
    "        print(\"No product matched your query.\")\n",
    "        return\n",
    "\n",
    "    # 1) get a big pool of neighbors (so later filters don’t starve us)\n",
    "    idxs, h, c, u = hybrid_scores(i, k=max(topn*50, 1000), alpha=alpha)\n",
    "\n",
    "    base_cat   = df.iloc[i][\"category_group\"]\n",
    "    base_price = df.iloc[i][\"price_num\"]\n",
    "    base_brand = df.iloc[i][\"brand\"]\n",
    "    query_key  = df.iloc[i][\"dedupe_key\"]\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # helper: matching rules\n",
    "    def ok_basic(j, require_cat=True, allow_price=True):\n",
    "        cond_brand = (df.iloc[j][\"brand\"] == base_brand) if same_brand else True\n",
    "        cond_cat   = (df.iloc[j][\"category_group\"] == base_cat) if require_cat else True\n",
    "        cond_price = (\n",
    "            True if not allow_price else\n",
    "            (pd.isna(base_price) or\n",
    "             pd.isna(df.iloc[j][\"price_num\"]) or\n",
    "             abs(df.iloc[j][\"price_num\"] - base_price) <= max(20, 0.5 * base_price))\n",
    "        )\n",
    "        if same_category_or_price:\n",
    "            cond_match = (cond_cat or cond_price)\n",
    "        else:\n",
    "            cond_match = True\n",
    "        return cond_brand and cond_match\n",
    "\n",
    "    # 2) primary candidate pool\n",
    "    pool = [j for j in idxs if ok_basic(j, require_cat=True, allow_price=True)]\n",
    "    if not pool:\n",
    "        pool = list(idxs)  # absolute fallback\n",
    "\n",
    "    # 3) optional similar-brands restriction, with variety fallback\n",
    "    if random_brand:\n",
    "        sim_brands = set(similar_brands(base_brand, topk=brand_topk))\n",
    "        pool_sim = [j for j in pool if (df.iloc[j][\"brand\"] in sim_brands)]\n",
    "        need_brands = max(1, int(np.ceil(topn / (max_per_brand or topn))))\n",
    "        uniq_brands = len({df.iloc[j][\"brand\"] for j in pool_sim})\n",
    "        if (len(pool_sim) >= topn) or (uniq_brands >= need_brands):\n",
    "            pool = pool_sim  # keep only similar brands if we still have enough variety\n",
    "\n",
    "    # 4) auto-relax if pool still too small → drop price, then category\n",
    "    if len(pool) < topn:\n",
    "        pool = [j for j in idxs if ok_basic(j, require_cat=True, allow_price=False)]\n",
    "    if len(pool) < topn:\n",
    "        pool = [j for j in idxs if ok_basic(j, require_cat=False, allow_price=False)]\n",
    "\n",
    "    # 5) dedupe + softmax sampling (for random_brand) + optional per-brand cap\n",
    "    keep, seen_keys = [], {query_key}\n",
    "    brand_counts = {}\n",
    "    cap = max_per_brand or topn  # unlimited if None\n",
    "\n",
    "    # align scores to pool\n",
    "    score_pos = {j: k for k, j in enumerate(idxs[:len(h)])}\n",
    "    def get_score(j):\n",
    "        p = score_pos.get(j, None)\n",
    "        return float(h[p]) if p is not None else 0.0\n",
    "\n",
    "    cand = np.array(pool, dtype=int)\n",
    "    scrs = np.array([get_score(j) for j in cand], dtype=float)\n",
    "\n",
    "    def softmax(x, T=1.0):\n",
    "        x = (x - x.max()) / max(T, 1e-6)\n",
    "        e = np.exp(x)\n",
    "        return e / (e.sum() + 1e-12)\n",
    "\n",
    "    while len(keep) < topn and len(cand) > 0:\n",
    "        if random_brand:\n",
    "            p = softmax(scrs, T=temperature)\n",
    "            pick_idx = rng.choice(len(cand), p=p)\n",
    "        else:\n",
    "            pick_idx = int(np.argmax(scrs))\n",
    "        j = int(cand[pick_idx])\n",
    "\n",
    "        k = df.iloc[j][\"dedupe_key\"]\n",
    "        b = df.iloc[j][\"brand\"]\n",
    "        if (k not in seen_keys) and (brand_counts.get(b, 0) < cap):\n",
    "            keep.append(j)\n",
    "            seen_keys.add(k)\n",
    "            brand_counts[b] = brand_counts.get(b, 0) + 1\n",
    "\n",
    "        cand = np.delete(cand, pick_idx)\n",
    "        scrs = np.delete(scrs, pick_idx)\n",
    "\n",
    "    # final fill if needed\n",
    "    if len(keep) < topn:\n",
    "        for j in pool:\n",
    "            if len(keep) >= topn: break\n",
    "            k = df.iloc[j][\"dedupe_key\"]; b = df.iloc[j][\"brand\"]\n",
    "            if (k in seen_keys) or (brand_counts.get(b,0) >= cap): continue\n",
    "            keep.append(j); seen_keys.add(k); brand_counts[b]=brand_counts.get(b,0)+1\n",
    "\n",
    "    # 6) assemble output\n",
    "    out = df.loc[keep, list(show_cols)].copy()\n",
    "    hybrid_full = dict(zip(idxs, h))\n",
    "    cont_full   = dict(zip(idxs, c))\n",
    "    coll_full   = dict(zip(idxs, u))\n",
    "    out.insert(0, \"score\",       np.round([hybrid_full.get(j, 0.0) for j in keep], 4))\n",
    "    out.insert(1, \"content_sim\", np.round([cont_full.get(j, 0.0)   for j in keep], 4))\n",
    "    out.insert(2, \"collab_sim\",  np.round([coll_full.get(j, 0.0)   for j in keep], 4))\n",
    "\n",
    "    print(\"Query →\", df.iloc[i][[\"product_name\",\"brand\",\"category_group\",\"price\"]].to_dict())\n",
    "    display(out.reset_index(drop=True))\n",
    "    return None\n",
    "\n",
    "print(\"✅ recommend_v2 is ready. Try:\")\n",
    "print(\" - recommend_v2('Airlift Intrigue Bra', 10, 0.7, same_brand=False, random_brand=True, brand_topk=20, max_per_brand=3, same_category_or_price=False, seed=42)\")\n",
    "print(\" - recommend_v2(0, 12, 0.6)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "38bf252b-3c60-4508-9378-7d3fdab16469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query → {'product_name': 'Airlift Intrigue Bra', 'brand': 'Alo Yoga', 'category_group': 'Activewear', 'price': 68.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>content_sim</th>\n",
       "      <th>collab_sim</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>category_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4124</td>\n",
       "      <td>0.4154</td>\n",
       "      <td>0.4053</td>\n",
       "      <td>Faux Leather Cropped Premier Bomber</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Outerwear</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7922</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.8089</td>\n",
       "      <td>Tennis Club Sweater Knit Skirt</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3848</td>\n",
       "      <td>0.3616</td>\n",
       "      <td>0.4388</td>\n",
       "      <td>Lauryl V Neck Sweater</td>\n",
       "      <td>Edikted</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4254</td>\n",
       "      <td>0.4170</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>Love Knots Tie Scrunchie</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3809</td>\n",
       "      <td>0.3555</td>\n",
       "      <td>0.4402</td>\n",
       "      <td>Wavy Baby Sequin Micro Shorts</td>\n",
       "      <td>Edikted</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.3563</td>\n",
       "      <td>0.4436</td>\n",
       "      <td>Steph Camo Pant</td>\n",
       "      <td>ALTAR'D STATE</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.3911</td>\n",
       "      <td>0.3697</td>\n",
       "      <td>0.4412</td>\n",
       "      <td>Vital V Neck Sports Bra</td>\n",
       "      <td>Gymshark</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3899</td>\n",
       "      <td>0.3687</td>\n",
       "      <td>0.4393</td>\n",
       "      <td>One Shoulder Sports Bra</td>\n",
       "      <td>Gymshark</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3811</td>\n",
       "      <td>0.3565</td>\n",
       "      <td>0.4384</td>\n",
       "      <td>Kasey Cable Knit Pants</td>\n",
       "      <td>Edikted</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3947</td>\n",
       "      <td>0.3747</td>\n",
       "      <td>0.4412</td>\n",
       "      <td>Vital Sports Bra</td>\n",
       "      <td>Gymshark</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    score  content_sim  collab_sim                         product_name  \\\n",
       "0  0.4124       0.4154      0.4053  Faux Leather Cropped Premier Bomber   \n",
       "1  0.7922       0.7850      0.8089       Tennis Club Sweater Knit Skirt   \n",
       "2  0.3848       0.3616      0.4388                Lauryl V Neck Sweater   \n",
       "3  0.4254       0.4170      0.4448             Love Knots Tie Scrunchie   \n",
       "4  0.3809       0.3555      0.4402        Wavy Baby Sequin Micro Shorts   \n",
       "5  0.3825       0.3563      0.4436                      Steph Camo Pant   \n",
       "6  0.3911       0.3697      0.4412              Vital V Neck Sports Bra   \n",
       "7  0.3899       0.3687      0.4393              One Shoulder Sports Bra   \n",
       "8  0.3811       0.3565      0.4384               Kasey Cable Knit Pants   \n",
       "9  0.3947       0.3747      0.4412                     Vital Sports Bra   \n",
       "\n",
       "           brand category_group  price  \n",
       "0       Alo Yoga      Outerwear  248.0  \n",
       "1       Alo Yoga     Activewear   98.0  \n",
       "2        Edikted     Activewear   28.0  \n",
       "3       Alo Yoga    Accessories   32.0  \n",
       "4        Edikted     Activewear   34.0  \n",
       "5  ALTAR'D STATE     Activewear   88.0  \n",
       "6       Gymshark     Activewear   38.0  \n",
       "7       Gymshark     Activewear   30.0  \n",
       "8        Edikted     Activewear   26.4  \n",
       "9       Gymshark     Activewear   38.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recommend_v2(\"Airlift Intrigue Bra\", 10, 0.7,\n",
    "             same_brand=False, random_brand=True,\n",
    "             brand_topk=20, max_per_brand=3,\n",
    "             same_category_or_price=False, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3c2ee3d3-8806-41f4-bf02-da898a7da6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query → {'product_name': 'Airlift Intrigue Bra', 'brand': 'Alo Yoga', 'category_group': 'Activewear', 'price': 68.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>content_sim</th>\n",
       "      <th>collab_sim</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>category_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8869</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>Airlift Empower Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8756</td>\n",
       "      <td>0.8943</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>Alosoft Relay Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8689</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>0.8069</td>\n",
       "      <td>Airlift Divine Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8688</td>\n",
       "      <td>0.8943</td>\n",
       "      <td>0.8094</td>\n",
       "      <td>Alosoft Molded Fantasy Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8631</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.8109</td>\n",
       "      <td>Slit Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.8631</td>\n",
       "      <td>0.8645</td>\n",
       "      <td>0.8597</td>\n",
       "      <td>Airlift Strength Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.8595</td>\n",
       "      <td>0.8529</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>Airlift Suit Up Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.8605</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>Splendor Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8483</td>\n",
       "      <td>0.8368</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>Wild Thing Bra</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.8470</td>\n",
       "      <td>0.8296</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>7/8 High-Waist Airlift Legging</td>\n",
       "      <td>Alo Yoga</td>\n",
       "      <td>Activewear</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    score  content_sim  collab_sim                    product_name     brand  \\\n",
       "0  0.8869       0.9266      0.7942             Airlift Empower Bra  Alo Yoga   \n",
       "1  0.8756       0.8943      0.8321               Alosoft Relay Bra  Alo Yoga   \n",
       "2  0.8689       0.8955      0.8069              Airlift Divine Bra  Alo Yoga   \n",
       "3  0.8688       0.8943      0.8094      Alosoft Molded Fantasy Bra  Alo Yoga   \n",
       "4  0.8631       0.8855      0.8109                        Slit Bra  Alo Yoga   \n",
       "5  0.8631       0.8645      0.8597            Airlift Strength Bra  Alo Yoga   \n",
       "6  0.8595       0.8529      0.8750             Airlift Suit Up Bra  Alo Yoga   \n",
       "7  0.8520       0.8605      0.8321                    Splendor Bra  Alo Yoga   \n",
       "8  0.8483       0.8368      0.8750                  Wild Thing Bra  Alo Yoga   \n",
       "9  0.8470       0.8296      0.8876  7/8 High-Waist Airlift Legging  Alo Yoga   \n",
       "\n",
       "  category_group  price  \n",
       "0     Activewear   98.0  \n",
       "1     Activewear   78.0  \n",
       "2     Activewear   68.0  \n",
       "3     Activewear   74.0  \n",
       "4     Activewear   68.0  \n",
       "5     Activewear   78.0  \n",
       "6     Activewear   78.0  \n",
       "7     Activewear   58.0  \n",
       "8     Activewear   78.0  \n",
       "9     Activewear  128.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recommend_v2(\"Airlift Intrigue Bra\", 10, 0.7,\n",
    "             same_brand=False, random_brand=False,\n",
    "             same_category_or_price=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f0a4ddc1-a80c-4fae-a836-5513cc83b0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/anaconda3/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q open_clip_torch pillow torchvision torch faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f7101fa3-5164-4fd4-9603-19e0d5f27b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached image embeddings: (7375, 512)\n"
     ]
    }
   ],
   "source": [
    "import os, io, json, time, math, hashlib, requests\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from torchvision import transforms\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"ViT-B-32\"\n",
    "PRETRAIN   = \"laion2b_s34b_b79k\"\n",
    "\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "    MODEL_NAME, pretrained=PRETRAIN, device=DEVICE\n",
    ")\n",
    "clip_model.eval()\n",
    "\n",
    "CACHE_PATH = \"image_embeddings_clip_vitb32.npy\"\n",
    "\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    image_embs = np.load(CACHE_PATH)\n",
    "    print(\"Loaded cached image embeddings:\", image_embs.shape)\n",
    "else:\n",
    "    imgs = [load_image_from_url(u) if isinstance(u, str) and u.strip() else None\n",
    "            for u in df[\"image_url\"]]\n",
    "    image_embs = embed_images(imgs, batch_size=32)\n",
    "    np.save(CACHE_PATH, image_embs)\n",
    "    print(\"Saved image embeddings:\", image_embs.shape)\n",
    "\n",
    "has_img = ~np.isnan(image_embs).any(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f48eecca-c4b5-4992-83ca-ad3443bd601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_sim_for_index(i, idxs):\n",
    "    \"\"\"Compute cosine similarity of image embeddings between item i and idxs.\"\"\"\n",
    "    if not has_img[i]:\n",
    "        return np.zeros(len(idxs))\n",
    "    base = image_embs[i] / np.linalg.norm(image_embs[i])\n",
    "    neigh = image_embs[idxs] / np.linalg.norm(image_embs[idxs], axis=1, keepdims=True)\n",
    "    return np.dot(neigh, base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "690782de-6f8d-4003-8080-c38ace8ba3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_scores_v2(i, k=100, alpha=0.6, beta=0.2, gamma=0.2):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        idxs, hybrid, content_sim, collab_sim, image_sim\n",
    "    \"\"\"\n",
    "    # reuse your existing hybrid_scores logic\n",
    "    idxs, content_sim, collab_sim = hybrid_scores(i, k=k, alpha=alpha)  \n",
    "    \n",
    "    # image similarity\n",
    "    image_sim = image_sim_for_index(i, idxs)\n",
    "\n",
    "    # combine weighted\n",
    "    hybrid = alpha * content_sim + beta * collab_sim + gamma * image_sim\n",
    "    return idxs, hybrid, content_sim, collab_sim, image_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bfd214b9-f871-490b-a01a-d2b5415cfb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_mm(query, topn=10, alpha=0.6, beta=0.2, gamma=0.2,\n",
    "                 show_cols=(\"product_name\",\"brand\",\"category_group\",\"price\"), **kwargs):\n",
    "    i = query if isinstance(query, int) else find_index_by_name(query)\n",
    "    if i is None:\n",
    "        print(\"No product matched your query.\")\n",
    "        return\n",
    "\n",
    "    idxs, h, c, u, img = hybrid_scores_v2(i, k=max(topn*50, 1000), alpha=alpha, beta=beta, gamma=gamma)\n",
    "\n",
    "    order = np.argsort(-h)[:topn]\n",
    "    idxs = [idxs[j] for j in order]\n",
    "\n",
    "    result = df.iloc[idxs][list(show_cols)].copy()\n",
    "    result[\"score\"] = h[order]\n",
    "    result[\"content_sim\"] = c[order]\n",
    "    result[\"collab_sim\"] = u[order]\n",
    "    result[\"image_sim\"] = img[order]\n",
    "\n",
    "    display(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ee3361ed-1fc0-4617-86ce-d90517533ffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m recommend_mm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAirlift Intrigue Bra\u001b[39m\u001b[38;5;124m\"\u001b[39m, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[132], line 8\u001b[0m, in \u001b[0;36mrecommend_mm\u001b[0;34m(query, topn, alpha, beta, gamma, show_cols, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo product matched your query.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m idxs, h, c, u, img \u001b[38;5;241m=\u001b[39m hybrid_scores_v2(i, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(topn\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1000\u001b[39m), alpha\u001b[38;5;241m=\u001b[39malpha, beta\u001b[38;5;241m=\u001b[39mbeta, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[1;32m     10\u001b[0m order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39mh)[:topn]\n\u001b[1;32m     11\u001b[0m idxs \u001b[38;5;241m=\u001b[39m [idxs[j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m order]\n",
      "Cell \u001b[0;32mIn[130], line 7\u001b[0m, in \u001b[0;36mhybrid_scores_v2\u001b[0;34m(i, k, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    idxs, hybrid, content_sim, collab_sim, image_sim\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# reuse your existing hybrid_scores logic\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m idxs, content_sim, collab_sim \u001b[38;5;241m=\u001b[39m hybrid_scores(i, k\u001b[38;5;241m=\u001b[39mk, alpha\u001b[38;5;241m=\u001b[39malpha)  \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# image similarity\u001b[39;00m\n\u001b[1;32m     10\u001b[0m image_sim \u001b[38;5;241m=\u001b[39m image_sim_for_index(i, idxs)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "recommend_mm(\"Airlift Intrigue Bra\", topn=10, alpha=0.6, beta=0.2, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1adb2e6-94cf-4dcb-8dd9-a119faa26e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
